<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Optimization Algorithms Part I | DLBlog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Optimization Algorithms Part I" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Some most commonly used optimization algorithms for updating network parameters, and their advantages/disadvantages." />
<meta property="og:description" content="Some most commonly used optimization algorithms for updating network parameters, and their advantages/disadvantages." />
<link rel="canonical" href="https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html" />
<meta property="og:url" content="https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html" />
<meta property="og:site_name" content="DLBlog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-28T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html","@type":"BlogPosting","headline":"Optimization Algorithms Part I","dateModified":"2020-10-28T00:00:00-05:00","datePublished":"2020-10-28T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html"},"description":"Some most commonly used optimization algorithms for updating network parameters, and their advantages/disadvantages.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DLblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nitishsadire.github.io/DLblog/feed.xml" title="DLBlog" /><link rel="shortcut icon" type="image/x-icon" href="/DLblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DLblog/">DLBlog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DLblog/about/">About Me</a><a class="page-link" href="/DLblog/search/">Search</a><a class="page-link" href="/DLblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Optimization Algorithms Part I</h1><p class="page-description">Some most commonly used optimization algorithms for updating network parameters, and their advantages/disadvantages.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-28T00:00:00-05:00" itemprop="datePublished">
        Oct 28, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/nitishsaDire/DLblog/tree/master/_notebooks/2020-10-28-DLforCNN.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/DLblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/nitishsaDire/DLblog/master?filepath=_notebooks%2F2020-10-28-DLforCNN.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/DLblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/nitishsaDire/DLblog/blob/master/_notebooks/2020-10-28-DLforCNN.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/DLblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Stochastic-Gradient-Descent-(SGD)">Stochastic Gradient Descent (SGD) </a></li>
<li class="toc-entry toc-h2"><a href="#SGD-+-Momentum">SGD + Momentum </a></li>
<li class="toc-entry toc-h2"><a href="#Nesterov-Momentum">Nesterov Momentum </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-10-28-DLforCNN.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>There are millions of parameters in a typical deep neural network, and these parameters are initialized with random values. Now as the training proceeds, as mentioned in <a href="https://nitishsadire.github.io/DLblog/jupyter/2020/10/14/DLforCNN.html">the previous post</a>, we fed the training examples into the network and compute loss using a loss function, and take gradients of loss w.r.t. all the parameters so as to update them. Now with the updated network parameters we again fed the training data, and this process repeats time and again until the loss converges. The part where we update the parameters of the network is the focus of today's post.</p>
<p>To update a parameter means to change it's value so as to minimize the loss we incurred. It is an optimization problem where the objective function is the loss function $L(w)$, $w$ is the parameter we are updating, and ideally, we want to find the global minima of loss function $L$ for parameter w, if such global minima exist, otherwise find local minima. One such method to find a minima. as mentioned in <a href="https://nitishsadire.github.io/DLblog/jupyter/2020/10/14/DLforCNN.html">the previous post</a>, as Gradient Descent (GD) also known as Vanilla Gradient Descent, where we take a small step in the direction of the negative gradient ($-\frac{\partial{L}}{\partial{w}}$). There are numerous issues with GD, like:</p>
<ol>
<li>Too much memory is required to feed all the training examples at once to the model.</li>
<li>What if we reached a saddle point (a point which is not local extremum but having gradient 0, please refer <a href="https://en.wikipedia.org/wiki/Saddle_point">wikipedia</a>), then gradient will be zero there and no updation will happen. </li>
</ol>
<p>Because of such issues, it's hardly used in practice. Other optimization algorithms have fundamental ideas the same as gradient descent but they encounter the issues faced by it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Stochastic-Gradient-Descent-(SGD)">
<a class="anchor" href="#Stochastic-Gradient-Descent-(SGD)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Gradient Descent (SGD)<a class="anchor-link" href="#Stochastic-Gradient-Descent-(SGD)"> </a>
</h2>
<p>It's a computational improvement over gradient descent as rather than feeding the whole training data it feeds data in batches. So if there are n training examples, then the batch size $b$ could be $1\leq b \leq n$. Batch-size is chosen based on memory availability. Mathematically,</p>
<p>$x_{t+1} = x_t - \alpha.\frac{\partial{L}}{\partial{x_t}}$</p>
<p>Some of the issues with SGD are:</p>
<ul>
<li>Not whole data is fed at once, so gradients will be noisy. By noisy I mean that because each batch will have a different set of examples, so they try to change the parameters in different directions, therefore there won't be a smooth curve to the minima, as shown below.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/DLblog/images/copied_from_nb/images/blog3_1.png" alt="" title="Credit:cs231n"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>What if the rate of change of loss is different in different directions. For example, as shown below, the contours of loss function form an ellipse, and function changes faster along the y-axis than the x-axis. Therefore the magnitude of the gradient along the y-axis will be more than the x-axis, and so the net direction of the gradient will align more with the y-axis. Therefore, the negative of the gradient will also align more with the y-axis, and hence the SGD will follow a zigzag path, which will take longer to converge.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/DLblog/images/copied_from_nb/images/blog3_2.png" alt="" title="Credit:cs231n"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Issues with saddle points still exists.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SGD-+-Momentum">
<a class="anchor" href="#SGD-+-Momentum" aria-hidden="true"><span class="octicon octicon-link"></span></a>SGD + Momentum<a class="anchor-link" href="#SGD-+-Momentum"> </a>
</h2>
<p>In SGD with Momentum, at any time t+1, the direction is not determined by $x_t$, but all the gradients from the beginning. Mathematically, if $v_0 = 0$ then,</p>
<p>$v_{t+1} = \rho v_t + \frac{\partial{L}}{\partial{x_t}}$</p>
<p>$x_{t+1} = x_t - \alpha.v_{t+1}$</p>
<p>typically, $\rho$=0.9, or 0.99. $v_t$ is known as velocity which is actually the running mean of gradients. Velocity at a point has exponentially decaying weighted sum of previous gradients (older the gradient is less is its weight). Therefore it's less noisy than SGD because gradients of other batches are also added.</p>
<p>Also in case of different rates of change of loss along with different dimensions, velocity gives better direction, plus the dimension along which there is less rate of change of loss function will have better velocity than in a normal SGD. So convergence will be faster as the path will be less zigzag. The velocity increases for dimension whose gradients point in the same direction and reduces for dimensions whose updates are in different directions. Therefore there will be less oscillation while converging.</p>
<p>It won't get stuck in saddle point because of velocity. Also, it will overshoot local minima but will come back and approach the minima again.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Nesterov-Momentum">
<a class="anchor" href="#Nesterov-Momentum" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nesterov Momentum<a class="anchor-link" href="#Nesterov-Momentum"> </a>
</h2>
<p>One issue with SGD+Momentum is that the gradients could accumulate which leads to a high gradient in a particular direction, which is not good because it could surpass the local minima, also convergence will be slower as it has to move back and forth to reach a local minima. Imagine like a ball is running downhill, then SGD+Momentum could be thought of as ball accumulating very high velocity, so could surpass a local minima and will try to climb an uphill. 
Nesterov Momentum update tries to correct this "blindly following the slope" nature of SGD+Momentum by calculating the slope of the future point, and moving in the -ve of net gradient direction. Mathematically,</p>
<p>$v_{t+1} = \rho v_t + \alpha\nabla_{x_t}L(x_t - \rho v_t)$</p>
<p>$x_{t+1} = x_t - v_{t+1}$</p>
<p>So here, firstly gradient of loss function L w.r.t to $x_t$ is computed at the approximate future point $x_t - \rho v_t$ (it's approximate because actual future point is $x_t - \rho v_t -$ gradient_of_L_w.r.t_x_t), so we have gradient knowledge of the next point. Now we add accumulated velocities to it and make an update on x.</p>
<p>This anticipatory update prevents us from going too fast and results in better updates, which has significantly increased performance on DL models. Now, we are able to adapt our updates to the slope of our error function and speed up SGD in turn.</p>
<p>SGD+Momentum and Nesterov Momentum updates are shown below in the figure. SGD+M computes gradient first (small blue vector) and then takes a step in the net direction of -ve accumulated gradients (big blue vector). Whereas, Nesterov goes in the direction of previously accumulated gradients (big brown vector), then finds gradient at approximate next point, and takes a step in -ve gradient direction (small red vector), so that the resultant direction (big green vector) is different from SGD+M.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/DLblog/images/copied_from_nb/images/blog3_3.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>We have covered Vanilla GD, SGD, SGD+Momentum, and Nesterov Momentum algorithms. All these optimization algorithms need a fine learning rate $\alpha$. If $\alpha$ too big then it will surpass the local minima or even diverge, if it's too small then convergence will be very slow. In Part II, we'll look at adaptive optimization algorithms, that will manage the learning rate for every parameter by itself, which is a very huge benefit over non-adaptive algorithms we have covered so far.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="nitishsaDire/DLblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/DLblog/2020/10/28/DLforCNN.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DLblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DLblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DLblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
