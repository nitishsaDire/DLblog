{
  
    
        "post0": {
            "title": "Numerical Optimisation",
            "content": "Introduction . In a deep learning software, there is high amount of numerical computation involved. The deeper a network gets more intesive the amount of numerical computation becomes. The rounding error at a one deeper layer could get amplified for the later layers, making the network learning wrong paramters. Because of the real numbers, underflow and overflow are two common issues occuring, for example in a softmax computation if value of $x_i$ is large positive number then $e^{x_i}$ could overflow, similarly if it&#39;s large negative number then $e^{x_i}$ could underflow. To overcome this, we subtract the max of $x$ from all $x_i$ and then only take softmax. . Gradient Based Optimisation . If there is a function $f(x)$, and we want to find value of $x$ where $f(x)$ is minimum, then calculus tells us that it happens where derivative or gradient of $f(x)$ w.r.t. $x$ is zero as shown in fig below. Gradient could be thought as linearly approximating the function $f(x)$ as $x$ changes to $x+ epsilon$ (bad approximation for large $ epsilon$): . $f(x+ epsilon) approx f(x) + epsilon .f&#39;(x)$ . . Deep learning involves optimisation, but the function is very complex such that it is not possible to get minima in a single step, so rather an iterative approach is followed, as we have discussed in Gradient Descent algorithm. Here, when I said &quot;minima&quot; I meant global minima, but there is also local minima, point where value of function is less than the value at neighbouring points. In general, the points where grad is zero are called as critical points, it could be any of the local/global minima maxima, saddle point (local minima along one direction and local maxima along other direction). In high dimensional functions (which are common in DL) there are lot of saddle points, local minima, and that what makes optimisation very difficult. . DIrection Derivative . Directional derivative is the measure of gradient along a certain direction. E.g. if $f(x)$ is the function involved, then directional derivative along the direction $u$ is derivative of $f(x+ alpha u)$ w.r.t $ alpha$ measured at $ alpha$=0. We know $ frac{ partial f(x+ alpha u)}{ partial alpha}$ is $u^T nabla f(x)$ at $ alpha=0$. Therefore, the direction of steepest descent (direction along which grad is decreasing fastest) will be when $u$ points in the opposite direction of $ nabla f(x)$, and this is the fundamental idea behind gradient descent. . Hessian . Hessian is the derivative of gradient, so if gradient is d-dim vector, measuring change of function along each of the d directions, then hessian is the dxd matrix, where ij entry measuring change of gradient along direction i with little change along direction j. Hessian measures curvature of the function, and there could be positive (upward), negative (downward) or no curvature (straigh line). Therefore the value of $f(x)$ decreases more than expected, decreases as expected and increases for the curvature negative, zero, positive respectively, when we make step of $ epsilon$ along a negative gradient direction, as shown below. . . The Hessian matrix is symmetric, means there exists orthogonal eigen-vectors and real eigen-values. The second directional derivative along direction d is $d^THd$. This is very interesting result because if d is choosen to be a unit vector then the value of $d^THd$ will be maximum/minimum when d is eigenvector corresponding to maximum/minimum eigenvalue, and for all the directions other than those in eigenvectors its value could be writen as some combination of value for eigenvectors (like a vector is respresented as linear combination of the basis). . Using taylor series quadratic approximation: . $f(x - epsilon g) approx f(x) - epsilon g^Tg + frac{1}{2} epsilon^2g^THg$ . When $g^THg$ is zero or negative then higher is the step size $ epsilon$ lesser will the $f(x)$. When $g^THg$ is positive then optimal step size could be derived. . The value of grad along with value of hessian is used to confirm whether the critical point is local minima/maxima, or a saddle point. . At a critical point one of the following things could happen: . If all the eigenvalues of hessian matrix are greater than zero, means the value of $d^THd$ is positive along all the directions, therefore the point is local minima because it has positive curvature (upwards) along all the directions. . | If all the eigenvalues of hessian matrix are less than zero, means the value of $d^THd$ is negative along all the directions, therefore the point is local maxima because it has negative curvature (downwards) along all the directions. . | If some of the eigenvalues are positive and some are negative then it means along one direction the curvature is upwards, and along other direction curvature is downwards, therefore the point is a saddle point. . | The condition number of the matrix is defined as the ratio of largest absolute eigenvalue to the smallest absolute eigenvalue. For an hessian it tells how much the second directional derivatives differ along different directions. When condition number is high then gradient descent performs poorly because the gradient along one direction (aligning with maximum eigenvalue&#39;s eigenvector) changes rapidly and along other direction (aligning with miniimum eigenvalue&#39;s eigenvector) changes slowly. This makes difficult to choose stepsize, and therefore gradient descent converges in a zig-zag manner. .",
            "url": "https://nitishsadire.github.io/DLblog/jupyter/2020/12/29/DLforCNN.html",
            "relUrl": "/jupyter/2020/12/29/DLforCNN.html",
            "date": " • Dec 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Batch Normalization",
            "content": "Introduction . When training a deep Neural Network (NN) then it&#39;s not that easy to just add more layers so as to improve the accuracy of the model. Deeper NN has its own challenges such as vanishing/exploding gradients, commonly said as poor grad flow. . There is one more issue with deeper NN, known as Internal Covariate Shift, which could be defined as the change in the distribution of the input of some layer due to a change in network parameters. When a NN updates its parameters of say Layer 1, then the input received by later layers could easily change in distribution, and it becomes difficult for later layers parameters to keep adapting to new distributions. The paper on Batch Normalization resolves this challenge by fixing the distribution of all the inputs for all the layers. The motivation is from the fact that if the input to the first layer is whitened then a NN converges faster with better accuracies. If the input to the hidden layers is also whitened then the whole NN will have similar gains. . Batch Normalization . With no whitening, problems of vanishing/exploding gradient generally occur for the deeper layers when the inputs to them have high or low magnitude. E.g. in sigmoid, the activations have zero grad when inputs are higher or lower, which is usually the case for deeper layers. One solution is to use RELU which does not have saturating outputs, but we want to develop a method instead which will put the inputs in a certain unsaturated range for all the activation functions. . Normalization could be done both before and after non-linearity, it&#39;s said to be beneficial to do before. . Whitening is an expensive process because we need to rotate the input by the eigenvector matrix, which needs to be calculated for every layer, every epoch. So instead, we will be normalizing the inputs per dimension, and instead of using the whole training dataset for calculating mean and variance per dimension, we will be estimating it per minibatch. . When we normalize the input of a layer we might end up decreasing the representation power of the layer because what if it wants the output to be in the saturated region in the case of sigmoid for example. So we scale and shift the normalized inputs. Algorithm-1 for BN is shown in the figure below, so any layer which was receiving input x will receive BN(x). Once the network is trained, we again input the whole training dataset to estimate the expected value of the mean and variance for all the layers, after which the network is ready for the inference. . For a CNN, if the feature map is of shape $p times q$ and there are m batches, then normalization is done as if there are n.p.q values, each of c (number of channels) dimensions. . . CNN with BN vs without BN . Let&#39;s define two CNNs, one using Batch Normalization before the Sigmoid layer, one without Batch Normalization. This will tell the impact of BN over the course of training. The dataset to use will be MNIST. . from fastai.callback.hook import * . path = untar_data(URLs.MNIST) . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, get_y=parent_label) dls = mnist.dataloaders(path/&#39;training&#39;) xb,yb = first(dls.train) xb.shape, yb.shape . ((64, 1, 28, 28), (64,)) . def block(ni, nf, stride=2, isBN=False): return ConvLayer(ni, nf, stride=stride, norm_type=None if isBN==False else NormType.Batch, act_cls=nn.Sigmoid) . def get_model(isBN): basic_cnn = nn.Sequential( block(1, 8, isBN=isBN), block(8, 16, isBN=isBN), block(16,32, isBN=isBN), block(32, 64, isBN=isBN), block(64, 128, isBN=isBN), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(128, 10) ) return basic_cnn . get_model(isBN=False) . Sequential( (0): ConvLayer( (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): Sigmoid() ) (1): ConvLayer( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): Sigmoid() ) (2): ConvLayer( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): Sigmoid() ) (3): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): Sigmoid() ) (4): ConvLayer( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): Sigmoid() ) (5): AdaptiveAvgPool2d(output_size=1) (6): Flatten(full=False) (7): Linear(in_features=128, out_features=10, bias=True) ) . get_model(isBN=True) . Sequential( (0): ConvLayer( (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): Sigmoid() ) (1): ConvLayer( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): Sigmoid() ) (2): ConvLayer( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): Sigmoid() ) (3): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): Sigmoid() ) (4): ConvLayer( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): Sigmoid() ) (5): AdaptiveAvgPool2d(output_size=1) (6): Flatten(full=False) (7): Linear(in_features=128, out_features=10, bias=True) ) . To get an idea what are the typical inputs, i.e what is input distribution to any of the sigmoid in the deeper layer, I have implemented a Hook callback. This hook will call before_fit at the start of fit initializing 3 lists for 15, 50, 85 percentile in the batch of inputs it has received. In after_batch it will append in the lists. get_stats is called in after_fit which will plot the stats with a running mean of 20. . class TstCallback(HookCallback): def before_fit(self, **kwargs): self.neuron_idx = 4 m = [self.model[-2]] super().__init__(**kwargs) self.stats=[] self.stats_15=[] self.stats_85=[] self.modules = m if self.every is None: self._register() def after_fit(self): self.get_stats() super().after_fit() def hook(self, m, i, o): return o def after_batch(self, **kwargs): if self.training and (self.every is None or self.train_iter%self.every == 0): self.stats.append(self.hooks.stored[0][:, self.neuron_idx].median().numpy()) self.stats_15.append(np.percentile(self.hooks.stored[0][:, self.neuron_idx], 15)) self.stats_85.append(np.percentile(self.hooks.stored[0][:, self.neuron_idx], 85)) super().after_batch() def get_stats(self, **kwargs): sts = [x.tolist() for x in self.stats] sts_15 = [x.tolist() for x in self.stats_15] sts_85 = [x.tolist() for x in self.stats_85] fig, ax = plt.subplots(figsize=(8,5)) N=20 ax.plot(range(len(sts)-N+1), self.running_mean(sts, N), label=&#39;50th&#39;) ax.plot(range(len(sts_15)-N+1), self.running_mean(sts_15, N), label=&#39;15th&#39;) ax.plot(range(len(sts_85)-N+1), self.running_mean(sts_85, N), label=&#39;85th&#39;) ax.legend() def running_mean(self, x, N): return np.convolve(x, np.ones(N)/N, mode=&#39;valid&#39;) . def get_learner(model): return Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=accuracy, cbs=[ActivationStats(with_hist=True),ShowGraphCallback(), TstCallback()] ) . Let&#39;s train a CNN without BN, by passign isBN=False in the get_model function. . learn = get_learner(get_model(isBN=False)) . learn.lr_find() . SuggestedLRs(lr_min=0.00020892962347716094, lr_steep=6.918309736647643e-06) . learn.fit_one_cycle(5, 7e-4) # Without BN . epoch train_loss valid_loss accuracy time . 0 | 2.304413 | 2.306328 | 0.109333 | 00:57 | . 1 | 2.306695 | 2.302060 | 0.109333 | 00:56 | . 2 | 2.297932 | 2.295692 | 0.109333 | 00:57 | . 3 | 1.749903 | 1.730008 | 0.423167 | 00:57 | . 4 | 1.613977 | 1.619583 | 0.485333 | 00:57 | . learn.activation_stats.plot_layer_stats(-2) . There are three plots generated, plot-1 plotting losses, whereas plot-2 has the various stats plot, plot-3 has activation stats for the last layer. These plots meanings are defined below: . plot-1: It the commonly used train/valid losses vs iterations plot.&gt; plot-2: It plots the different percentiles of the input for any sigmoid in the penultimate layer. &gt; plot-3: It plots the mean, standard deviation, and percent-near-zeroof the activation output of the penultimate layer. . We will discuss these plots once we also have got similar plots for CNN with Batch Normalization. . learn_bn=get_learner(get_model(isBN=True)) learn_bn.lr_find() . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=0.02754228748381138) . learn_bn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.160031 | 0.732354 | 0.793917 | 01:01 | . 1 | 0.105723 | 0.212742 | 0.935333 | 01:01 | . 2 | 0.070892 | 0.083195 | 0.973167 | 01:01 | . 3 | 0.042814 | 0.039509 | 0.988667 | 01:00 | . 4 | 0.022301 | 0.034257 | 0.990167 | 01:01 | . learn_bn.activation_stats.plot_layer_stats(-2) . Differences . We compare below the three plots of both the CNNs: . plot-1: This plot tells that CNN with BN is clearly having faster convergence, better accuracies. The accuracy achieved is 0.99 in just 5 epochs, whereas on CNN without BN it&#39;s 0.48 in the same number of epochs. The CNN without BN might able to achieve comparable accuracies if we train it for more epochs but its convergence is definitely slower than CNN with BN. . plot-2: The distribution of inputs to any sigmoid in CNN with BN remains stable throughout the training, which helps the network learn faster. In CNN without BN, the distributions keep on changing, because of which the network has to keep adapting to new input distributions, and hence slower convergence. . plot-3: The main subplot of interest is %-near-zero which tells the percentage of activations that are near to zero. Clearly, CNN with BN has fewer such activations compared to CNN without BN. . Key Takeaways . Batch normalization prevents small changes in network parameters to get amplify for the later layers. | It allows the use of higher learning rates which gives faster convergence, better accuracies because otherwise, the network might get stuck in poor local minima. With high learning rates, the model tends the find the large flat surfaces of the loss function which has better generalization power than a sharp local minima. Using high learning rates won&#39;t be possible otherwise because of vanishing/exploding gradients. | Because the inputs are normalized for each layer, therefore scale doesn&#39;t matter, i.e. BN(Wu) = BN(aWu) | Batch normalization regularizes the model because at every epoch the mini-batches are randomly created, and so the estimates of mean and variance of every layer have a lot of variations. These variations make the layers robust to changes in the input which produces a regularization effect. | Batch normalization adds only 2 parameters per layer, but the representation power of the network is the same as before. |",
            "url": "https://nitishsadire.github.io/DLblog/jupyter/2020/12/22/DLforCNN.html",
            "relUrl": "/jupyter/2020/12/22/DLforCNN.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction to Convolutional Neural Network (CNN) PART III",
            "content": "Introduction . We have seen the working of a simple CNN on the MNIST dataset in previous posts. We have also noticed that CNN gives far better accuracy than a simple Neural Network on image-related tasks. For more complex datasets we need to have much deeper CNNs, with layers even &gt;100. The issue with deeper networks is that gradient flow during backpropagation is poor and so it needs additional Batch Normalisation (I will explain it in a post dedicated to BN) layers. Even after adding BN layers to smooth the grad flows it&#39;s been noticed that deeper NN have less training/validation accuracies than their shallower counterparts. . If we think in terms of the power of fitting a complex function by a NN, we can say that this power increases when a network gets deeper. Suppose a shallow NN is able to achieve some accuracy for a problem then there exists a solution by construction to the deeper NN where the added layers are identity mappings and other layers are the same as learned by shallow NN. The existence of the solution by construction means that any deeper NN shouldn’t have more training/Val loss than the shallower one. . The motivation of ResNet is that the deeper NN is having difficulty in learning identity mappings (same output by a layer as input) because if it was able to learn it then the deeper NN should have at least the accuracy of shallower NN. To ease the learning of identity mappings shortcut connections are added. If say there are few layers stacked, and the desired mapping is $ mathcal{H}(x)$, where $x$ is the input to the first layer. Instead of letting the layers fit the $ mathcal{H}(x)$, we let these layers fit a residual mapping $ mathcal{F}(x) = mathcal{H}(x) - x$, and then the x from input is added to this new mapping. Adding x to the output of the next 2-3 layers is done by adding a shortcut connection. Introducing shortcut connections after every 2-3 layers adds no extra parameter to the NN. . With the addition of shortcut connection, learning identity mappings is easier because now the NN has to just push the weights of layers to 0, and experiments indicate that it&#39;s easier to do this than the earlier case. . Dataset . We have used MNIST dataset in the last post, but that dataset is very simple, so even without much effort, we got 0.99 accuracy. For ResNets, to see their potential we will train and predict on a much more complex dataset, the Imagenette. . In the below function get_data(), it returns a data loader with bs=128. It&#39;s taken from the fast.ai tutorials, I will explain the augmentations done in item_tfms (item transformations) and batch_tfms (batch transformations) in some later post. These augmentations are very commonly done in CNN datasets. Using them we create (randomly) more training data images, which are used to avoid overfitting. . def get_data(url, presize, resize, bs=128): path = untar_data(url) return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(presize), batch_tfms=[*aug_transforms(min_scale=0.5, size=resize), Normalize.from_stats(*imagenet_stats)], ).dataloaders(path, bs=bs) . dls = get_data(URLs.IMAGENETTE_160, 160, 128) . dls.show_batch() . print(f&#39;shape of 1 batch is {first(dls.train)[0].shape} and labels shape is {first(dls.train)[1].shape}&#39;) . shape of 1 batch is (128, 3, 128, 128) and labels shape is (128,) . Each image is a grid of 128x128 with 3 channels. Now we can&#39;t simply use the simplistic approach of the last post&#39;s CNN where we keep on using stride=2 until the grid size becomes very small or becomes 1x1 because: . If the image size is big it will take a lot of layers. | It won&#39;t accept any input image of other dimensions. | To tackle point-1 we can use AdaptiveAvgPool2d after sometime. Below I have implemented a simple stride 2 CNN (which keeps halving the grid). All Convolutional Layers are simply using stride-2 on the input and doubling the number of channels. As we will see such a simple architecture won&#39;t give us good precision. . def block(ni, nf): return ConvLayer(ni, nf, stride=2) def get_model(): return nn.Sequential( block(3, 16), # 64 x 64 x 16 block(16, 32), # 32 x 32 x 32 block(32, 64), # 16 x 16 x 64 block(64, 128), # 8 x 8 x 128 nn.AdaptiveAvgPool2d(1), # 1 x 1 x 128 Flatten(), # 128 nn.Linear(128, dls.c)) . learn = Learner(dls, get_model(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=0.005248074419796467) . learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.102683 | 1.956923 | 0.328662 | 00:23 | . 1 | 1.823126 | 1.835669 | 0.370191 | 00:22 | . 2 | 1.626245 | 1.624469 | 0.471338 | 00:23 | . 3 | 1.514813 | 1.489317 | 0.524841 | 00:23 | . 4 | 1.463456 | 1.472536 | 0.534522 | 00:22 | . ResNet . Let’s create a block of ResNet. In the below code, the block is of 2 conv layers, and in forward pass, we add the input x with the output of the block and pass it through an activation function. . class ResNetBlock(Module): def __init__(self, ni, nf, stride=1): self.block = nn.Sequential(ConvLayer(ni, nf, 3, stride=stride), ConvLayer(nf, nf, 3, act_cls=None)) def forward(self, x): return F.relu(x + self.block(x)) . There is one very drastic problem here, we have to have ni==nf, and stride=1, otherwise the x and output of the block won&#39;t have the same dimensions and therefore won&#39;t add. To overcome this issue we can add a projection matrix when ni!=nf and a pooling layer when stride=2. . class ResNetBlock(Module): def __init__(self, ni, nf, stride=2): self.block = nn.Sequential(ConvLayer(ni, nf, 3, stride=stride), ConvLayer(nf, nf, 3, act_cls=None)) self.proj = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2) def forward(self, x): return F.relu(self.proj(self.pool(x)) + self.block(x)) . def block(ni, nf, stride=2): return ResNetBlock(ni,nf,stride) . learn = Learner(dls, get_model(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=0.006918309628963471, lr_steep=0.0063095735386013985) . learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.880656 | 1.725776 | 0.423440 | 00:23 | . 1 | 1.545003 | 1.477175 | 0.511338 | 00:23 | . 2 | 1.312130 | 1.232368 | 0.598217 | 00:23 | . 3 | 1.156471 | 1.084143 | 0.654267 | 00:23 | . 4 | 1.066587 | 1.072679 | 0.662675 | 00:23 | . We have been able to achieve far better accuracy of 0.66 with ResNets as compared to 0.534 accuracy of regular CNN. But can we do better, yes we can. We can build deeper ResNets (with some tweaks to help in deeper models). . In any CNN, the number of computations in the initial layers is more than in the deeper layers, and the number of parameters in the deeper layers is more than in the initial layers. So that&#39;s why the initial layers of deep CNNs are kept relatively fast and simple, and these initial layers are known as the stem. . def stem(sizes): return [ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1) for i in range(len(sizes) - 1) ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] . class ResNetModel(Module): def __init__(self, n_layers=[2,2,2,2], stem_sizes=[3,32,64], expansion=1): self.stem = stem(stem_sizes) self.channels = [64,64,128,256,512] for i in range(1, 5): self.channels[i] *=expansion # print(self.channels) # print(n_layers) self.blocks = [add_multi_blocks(self.channels[i], self.channels[i+1], i, n_l) for i,n_l in enumerate(n_layers)] self.model = nn.Sequential(*self.stem, *self.blocks, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(512*expansion,10)) def forward(self, x): return self.model(x) def add_multi_blocks(ci, co, idx, n_blocks): # print(idx, n_blocks) stride = 1 if idx==0 else 2 return nn.Sequential(*[block(ci if i==0 else co, co, stride if i==0 else 1) for i in range(n_blocks)]) . learn = Learner(dls, ResNetModel(), loss_func=nn.CrossEntropyLoss(), metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=0.0019054606556892395, lr_steep=0.00015848931798245758) . Now because the model is fairly deep with 26 layers, we will use more epochs. . learn.fit_one_cycle(20, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.651197 | 1.537189 | 0.488662 | 00:27 | . 1 | 1.341704 | 2.647639 | 0.349299 | 00:27 | . 2 | 1.185382 | 1.601156 | 0.534013 | 00:27 | . 3 | 1.105674 | 1.118512 | 0.644586 | 00:27 | . 4 | 1.010274 | 1.173519 | 0.630573 | 00:27 | . 5 | 0.913442 | 1.576603 | 0.556178 | 00:27 | . 6 | 0.853161 | 1.416359 | 0.603312 | 00:27 | . 7 | 0.779800 | 1.032142 | 0.686115 | 00:27 | . 8 | 0.718172 | 0.864432 | 0.727643 | 00:27 | . 9 | 0.685136 | 1.101193 | 0.666242 | 00:27 | . 10 | 0.629721 | 0.773303 | 0.752357 | 00:27 | . 11 | 0.566024 | 0.620527 | 0.799490 | 00:27 | . 12 | 0.531753 | 0.771543 | 0.766115 | 00:27 | . 13 | 0.488256 | 0.631772 | 0.809682 | 00:27 | . 14 | 0.436432 | 0.544485 | 0.824968 | 00:27 | . 15 | 0.385509 | 0.554845 | 0.830064 | 00:27 | . 16 | 0.341558 | 0.570452 | 0.824968 | 00:27 | . 17 | 0.304268 | 0.499487 | 0.848408 | 00:27 | . 18 | 0.287553 | 0.501760 | 0.848408 | 00:27 | . 19 | 0.271609 | 0.480697 | 0.855541 | 00:27 | . That&#39;s a huge improvement in accuracy. Can we do any better with using deeper ResNets, let&#39;s see. . Bottleneck ResNet Architecture . For very deep ResNets, where the number of layers &gt; 50, the double convolutional layers of kernel size 3x3, and using a projection matrix to add input x to the output could be slow for such a big network. What we can do is replace it with 3 conv layers, one is reducing dimension with 1x1, conv layer 3x3, increasing dim to original with 1x1. We have increased the number of filters in the end so that we don&#39;t need a projection matrix to project input x to add to the output of the 3rd layer, which will reduce the number of parameters for deep CNNs. . class ResNetBlock_bottleneck(Module): def __init__(self, ni, nf, stride=1): self.block = nn.Sequential(ConvLayer(ni, nf//4, 1), ConvLayer(nf//4, nf//4, stride=stride),ConvLayer(nf//4, nf, 1, act_cls=None)) self.proj = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2) def forward(self, x): return F.relu(self.proj(self.pool(x)) + self.block(x)) def block(ni, nf, stride=1): return ResNetBlock_bottleneck(ni,nf,stride) . If we give n_layers=[3, 4, 6, 3] and expansion=4, the architecture is known as ResNet-50 and has a total of 50 layers. Let&#39;s also change the size of the input image to 160, because now we have deeper architecture. . dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=160, bs=64) . learn = Learner(dls, ResNetModel(n_layers=[3,4,6,3], expansion=4), loss_func=nn.CrossEntropyLoss(), metrics=accuracy) . learn.lr_find() . SuggestedLRs(lr_min=3.981071640737355e-05, lr_steep=0.25118863582611084) . learn.fit_one_cycle(20, 2e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.822623 | 2.223737 | 0.363822 | 01:34 | . 1 | 1.543590 | 1.883778 | 0.478981 | 01:35 | . 2 | 1.422131 | 2.530116 | 0.402548 | 01:36 | . 3 | 1.302013 | 1.682341 | 0.508790 | 01:36 | . 4 | 1.182231 | 1.523533 | 0.523312 | 01:36 | . 5 | 1.085471 | 1.266194 | 0.602293 | 01:36 | . 6 | 0.998128 | 2.586358 | 0.432611 | 01:36 | . 7 | 0.912897 | 0.959152 | 0.697325 | 01:37 | . 8 | 0.847739 | 0.980152 | 0.701656 | 01:36 | . 9 | 0.787929 | 0.992222 | 0.720764 | 01:36 | . 10 | 0.688518 | 0.654835 | 0.792866 | 01:37 | . 11 | 0.642279 | 0.685462 | 0.782675 | 01:37 | . 12 | 0.550949 | 0.731216 | 0.776051 | 01:37 | . 13 | 0.551117 | 0.593097 | 0.811465 | 01:36 | . 14 | 0.468444 | 0.544953 | 0.820892 | 01:37 | . 15 | 0.420975 | 0.490345 | 0.847389 | 01:37 | . 16 | 0.382126 | 0.434405 | 0.866497 | 01:37 | . 17 | 0.349571 | 0.412998 | 0.872611 | 01:37 | . 18 | 0.317281 | 0.423543 | 0.870573 | 01:37 | . 19 | 0.324792 | 0.423680 | 0.870573 | 01:37 | . This has further improved accuracy. We can try even deeper architectures by including the layers as shown in the image below. This figure is from the ResNet paper. . . Conclusion . We have implemented CNN from scratch on the MNIST model, ResNet on the Imagenette model. We have seen the improvements we get from using more sophisticated architectures. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/12/15/DLforCNN.html",
            "relUrl": "/2020/12/15/DLforCNN.html",
            "date": " • Dec 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Introduction to Convolutional Neural Network (CNN) PART II",
            "content": "Introduction . In the first part, we discussed the fundamentals of a CNN model. We learned how the convolution layer detects various patterns present in the input image. The later convolution layers detect more complex patterns, and so we generally keep on increasing the number of channels as we move to deeper layers. In this post, we will train a NN and CNN on the MNIST dataset and will compare the results. . MNIST Dataset . Lets load the MNIST dataset in to a dataloader. . (ImageBlock(cls=PILImageBW), CategoryBlock) Tells the input is an image, and output will be one category per input image. There are total 10 categories (0-9). . get_image_files gets the all image files recursively in the path mentioned. . parent_label labels the image by the name of parent directory the file is present in. All the images are present in subfolder named &#39;0&#39; to &#39;9&#39;. . path = untar_data(URLs.MNIST) . im3 = Image.open(path/&#39;training&#39;/&#39;3&#39;/&#39;12.png&#39;) show_image(im3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3f9e696940&gt; . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, get_y=parent_label) dls = mnist.dataloaders(path/&#39;training&#39;) xb,yb = first(dls.valid) xb.shape, yb.shape . ((64, 1, 28, 28), (64,)) . Batch-size is taken as default value 64, the input image is of dimension $1 times 28 times 28$. . print(f&#39;Total mini-batches in training dataset are {len(dls.train)}&#39;) print(f&#39;Total mini-batches in validation dataset are {len(dls.valid)}&#39;) . Total mini-batches in training dataset are 750 Total mini-batches in validation dataset are 188 . print(f&#39;Total classes = {dls.c}&#39;) . Total classes = 10 . Architecture . Convolutional Block: I have defined a function block() which takes 3 parameters explained as follows: . ni: This is the number of channels in the input image. | nf: This is the total number of filters or number of channels in the output image. | stride: It is the value of stride, default 2, which will be used while convolving filter on the input. | def block(ni, nf, stride=2): return ConvLayer(ni, nf, stride=stride) . CNN Model: The CNN model I have defined below continuously halves the input grid size by using a stride of two, and so it doubles the number of channels. After the fourth convolutional layer, the output is of size $4 times 4 times 64$, on it, adaptiveAvgPool2d is applied. It converts the 2x2 grid into the desired shape (passed as 1, therefore it will convert 2x2 into 1x1). The output of this will be of shape $1 times 1 times 64$ which is flattened and put into an fc layer. . NN Model A simple neural network model is also implemented which has 2 layers. . def get_model(): basic_cnn = nn.Sequential( block(1, 8), block(8, 16), block(16,32), block(32,64), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(64, 10) ) return basic_cnn def get_nn(): basic_nn = nn.Sequential( nn.Flatten(), nn.Linear(784, 512), nn.Linear(512, 10) ) return basic_nn . Let&#39;s create a learner that we will use to train both the models. . def get_learner(model): return Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=accuracy) . learn = get_learner(get_nn()) . Training . To find the optimal range of learning rate we plot graph of training loss for different lr starting from a small value. The range at which the rate of decrease of loss is sharpest is the range where if lr is taken from the convergence will be faster. We do not choose the lr corresponding to the minimum loss because no/very-slow learning from that point onwards. . learn.lr_find() . SuggestedLRs(lr_min=0.002290867641568184, lr_steep=0.0063095735386013985) . Firstly we will train nn model for 10 epochs, at a max learning rate of 2e-3. . fit_one_cycle() if fits for say 1 epoch, and there are 750 iterations in total, then it starts with a small lr, gradually goes to the max value provided (2e-3 in below code) in first roughly 20% iterations, and then keep on decreasing until the last iteration because later iterations need less lr. . learn.fit_one_cycle(10, 2e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.343918 | 0.334851 | 0.903333 | 00:35 | . 1 | 0.319538 | 0.352520 | 0.902667 | 00:37 | . 2 | 0.339889 | 0.378988 | 0.897333 | 00:39 | . 3 | 0.306681 | 0.351762 | 0.905000 | 00:39 | . 4 | 0.284653 | 0.323651 | 0.910000 | 00:40 | . 5 | 0.280786 | 0.322405 | 0.911750 | 00:41 | . 6 | 0.254253 | 0.307338 | 0.919750 | 00:41 | . 7 | 0.249282 | 0.295363 | 0.922333 | 00:39 | . 8 | 0.246942 | 0.292607 | 0.924833 | 00:39 | . 9 | 0.229630 | 0.292373 | 0.924583 | 00:40 | . The accuracy achieved by it is 0.925. Now we have seen results from a plain NN, let&#39;s try training a CNN and see the results. . learn = get_learner(get_model()) . learn.lr_find() . SuggestedLRs(lr_min=0.04365158379077912, lr_steep=0.0831763744354248) . learn.fit_one_cycle(10, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.119381 | 0.127926 | 0.961750 | 00:43 | . 1 | 0.092528 | 0.108258 | 0.966417 | 00:42 | . 2 | 0.073737 | 0.083760 | 0.974583 | 00:41 | . 3 | 0.064387 | 0.085716 | 0.973250 | 00:42 | . 4 | 0.041789 | 0.059548 | 0.983583 | 00:43 | . 5 | 0.032186 | 0.041528 | 0.987667 | 00:44 | . 6 | 0.021864 | 0.046303 | 0.987000 | 00:45 | . 7 | 0.008966 | 0.041324 | 0.988417 | 00:44 | . 8 | 0.005580 | 0.036766 | 0.989917 | 00:45 | . 9 | 0.002051 | 0.038017 | 0.990083 | 00:45 | . Nice, we have able to get an accuracy of 0.99 with a CNN, which is far better than a simple NN . Clearly, CNN is able to learn patterns of the images better than the plain NN. Also, the dataset is so simple that we are able to get more than 0.99 accuracy with a simple CNN. If the dataset is a complex one that this simple CNN will fail for sure. It will start overfitting, grads flow will be poor, and a lot of other issues. We will get to know more about this in the next and last part, and also we will see far more sophisticated architecture known as Residual Network (ResNet). Thanks. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/12/08/DLforCNN.html",
            "relUrl": "/2020/12/08/DLforCNN.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to Convolutional Neural Network (CNN) PART I",
            "content": "Introduction . In any ML/DL problem, we have seen so far, for example, the tabular modeling, there are different columns with a lot of values, and these columns are helping us to find a relationship that is used to predict the unseen data. But what if the data at hand is not tabular data, for example, images. Suppose we have MNIST hand-written digits dataset and want to predict which of the 10 digits is in the image. Each image is 28x28 and belongs to one of the 10 classes (0-9). One approach is we can flatten the array into a 784 length vector, and create a model that has some non-linear fully connected layers, and the last layer is softmax which gives the probability of the input image for the 10 classes. We will get some decent validation accuracy, but there are some fundamental issues with the approach. With fc layers, there are a lot of parameters involved, e.g., if the image is 500x500x3, it flattens to 750000 long vector, and say we map the input to 1000 length vector, which is not a big vector, then for the first layer itself there will be $75 times 10^8$, or 7.5 Billion parameters. Any NN with a lot of parameters has a tendency to overfitting because its expressive power is very large and so it even fits the noise in the training data. . Also, we are not using the presence of the different patterns in the image, for example, digit-7 is having one horizontal line towards the top, then an angled or vertical line, digit-3 is curvy on the top and similar patterns for the other digits. . In the CNN we do convolution operation on the input image with various kernels/filters as shown below. Here the bigger square is an input image, and the smaller square is a kernel, and convolution operation is shown where the kernel is placed on the red-region of the input image. . . Similarly, the kernel is moved one oy one on the whole image, and output is constructed having values of the convolution of the input image and kernel. E.g., In the image below, light blue is the input image, dark blue is a kernel that will be moved across all the locations in the input image, green is the output constructed whereas dark green shows which cells are involved in calculating the value. . . So if input image is of size $i times i$, kernel is of size $k times k$, then the output will be of size $(i-k+1) times (i-k+1)$. In the above image, the center $2 times 2$ square is involved in all the convolutions, and the corner 4 $1 times 1$ squares are involved only in a single convolution. This will give undue advantage to the pixels which are towards the center of the image. To counter this we can do padding of the image, i.e. adding zeros along both dimensions so that if the input image was $i times i$ it will become $(i+2) times (i+2)$. . Till now we are moving the kernel one pixel ahead or down at a time, we can move it two pixels at a time, and this value is known as stride. So in general, input image $i times i$, kernel $k times k$, padding is p and stride is s will produce output image of dimension $ left( frac{i + 2*p - k}{s} + 1, frac{i + 2*p - k}{s} + 1 right)$. . Exploring Kernels . A Kernel is used to explore different patterns in the input image. For example, we want to detect different edges, curves in the image which will help us in our prediction. Let’s explore some simple kernels on MNIST dataset. . path = untar_data(URLs.MNIST_SAMPLE) . im3 = Image.open(path/&#39;train&#39;/&#39;3&#39;/&#39;12.png&#39;) show_image(im3, cmap=&#39;Greys&#39;) print(f&#39;Tensor shape is {tensor(im3).shape}&#39;) . Tensor shape is torch.Size([28, 28]) . We will be using 4 kernels. The first one is top_edge_ld, which means top edge light to dark. Light pixels are closer to 0 and dark are closes to 255. Therefore, in order to detect horizontal edge which goes from light to dark regions our kernel has -1 in row-1, 0 in row-2, and +1 in row-3. It is because it will detect regions where above row has value closer to 0, and below rows have values closer to 255, so we need to row-1 smallest and row-3 largest so that the convolution gives a big number. . Similar logic goes for other 3 kernels. top_edge_dl goes from dark to light instead so it has row-1 &gt; row-3. vert_edge_ld and vert_edge_dl detect vertical lines which goes from light in left to dark in right, and dark in left to light in right respectively. . top_edge_ld = tensor([ [-1,-1,-1], [ 0, 0, 0], [ 1, 1, 1]]).float() top_edge_dl = tensor([ [ 1, 1, 1], [ 0, 0, 0], [-1,-1,-1]]).float() vert_edge_ld = tensor([ [-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).float() vert_edge_dl = tensor([ [ 1, 0,-1], [ 1, 0,-1], [ 1, 0,-1]]).float() kernels = torch.stack((top_edge_ld, top_edge_dl, vert_edge_ld, vert_edge_dl)) patterns = [&#39;hor ld&#39;, &#39;hor dl&#39;, &#39;ver ld&#39;, &#39;ver dl&#39;] . def apply_kernel(img, row, col, kernel): return (img[row-1:row+2,col-1:col+2] * kernel).sum() . rng = range(1,27) img = tensor(im3) for idx in range(4): top_edge3 = tensor([[apply_kernel(img,i,j,kernels[idx]) for j in rng] for i in rng]) show_image(top_edge3, cmap=&#39;Greys&#39;, title=patterns[idx] + &quot;, shape is: &quot; + str(top_edge3.shape)); . The output images are shown above along with their dimensions. All are of shape $26 times 26$ (28 - 3 + 1). The first one is the output of detecting horizontal edges that go from light to dark, and we can see that the darker regions in the output image corresponding to those areas, its because darker means higher values after convolution. In the next hor dl, it goes from dark to light instead and therefore the darker regions which are for higher magnitude are in the downward direction. Similar observations for vert ld and vert dl. . In a CNN, the output of applying kernel is applied a non-linear activation function (like we did in fully connected layers) so that non-linearity is introduced in the model. . CNN Architecture . If the input is 28$ times$28$ times$3, 3 because colored images have 3 channels of RGB, and we start with 4 3$ times$3$ times$3 filters (dimension of a filter is h$ times$w$ times$c, where c is the number of channels in the input image, and during convolution, the values across all the channels will be added to get a single value) in the first layer, then output from the first layer will be 14$ times$14$ times$4 with a stride of 2 and padding of 1, 64 because there were 64 filters each producing a 2-d output of size 14$ times$14. . In general, if kernel size is k, then using a stride of 2 and padding of k//2 gives an output image of half the size of the input image. . In the second layer, if we use 8 3$ times$3$ times$4 size kernels, with stride 2, outputting an image of 7$ times$7$ times$8. One important observation here is that we have used stride of 2, which will decrease the number of values in the output image by 4 (2 along each dimension), but we have also doubled the number of filters therefore the output is of size 7$ times$7$ times$8. Filters are doubled because we don&#39;t want to lose a lot of activations in the deeper layers. . We keep on going like this, halving the image size and doubling the number of channels at each layer. In the end, we have 1$ times$1$ times$32 output, which we could flatten and have an fc layer to map 32-dim input to 10-dim (number of classes). Lastly, we do softmax which will give a normalized probability of all the classes. . Following steps are applied: . 28$ times$28$ times$3, applied 4 3$ times$3$ times$3 filters. | 14$ times$14$ times$4, applied 8 3$ times$3$ times$4 filters. | 7$ times$7$ times$8, applied 16 3$ times$3$ times$8 filters. | 3$ times$3$ times$16, applied 32 3$ times$3$ times$32 filters. | 1$ times$1$ times$32 | . We will be implementing a CNN model to classify the MNIST dataset in the next part. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/12/01/DLforCNN.html",
            "relUrl": "/2020/12/01/DLforCNN.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "ML/DL techniques for Tabular Modeling PART III",
            "content": "Introduction . In this, the last part on tabular modeling, we will look into training a Neural Network (NN) on the Kaggle Dataset. In the last two parts we have seen decision trees, and random forests and look into their advantages/disadvantages. Their main advantage is their simplicity, faster training, and disadvantage being unable to extrapolate on out of domain data. Firstly let’s train a NN on the random dataset to examine its extrapolation power. . Extrapolation . x = np.linspace(0, 10, 110) y = x + np.random.randn(110) df_lin=pd.concat([pd.DataFrame(x, columns=[&#39;x&#39;]), pd.DataFrame(y, columns=[&#39;y&#39;])], axis = 1) cont_names=[&#39;x&#39;] cat_names=[] cond = (df_lin.x&lt;8) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) procs_nn = [Normalize] to_nn = TabularPandas(df_lin, procs_nn, cont_names=cont_names, cat_names=cat_names, splits=splits, y_names=[&#39;y&#39;]) dls = to_nn.dataloaders(30) y.min(), y.max() learn = tabular_learner(dls, y_range=(-2,13), layers=[100,10], n_out=1, loss_func=F.mse_loss) learn.fit_one_cycle(30, 5e-2) preds,targs = learn.get_preds(0) v_preds,v_targs = learn.get_preds(0) tr_res,_ = learn.get_preds(0) val_res,_ = learn.get_preds(1) df_lin.iloc[valid_idx] fig, ax = plt.subplots(figsize=(16,8)) ax.scatter(x,y, marker=&#39;+&#39;, label=&#39;actual data&#39;) ax.scatter(df_lin.iloc[train_idx][&#39;x&#39;], tr_res, label=&#39;training data prediction&#39;) ax.scatter(df_lin.iloc[valid_idx][&#39;x&#39;], val_res, label=&#39;validation data prediction&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;y&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:15}) . epoch train_loss valid_loss time . 0 | 7.972749 | 4.008810 | 00:00 | . 1 | 6.677185 | 2.662161 | 00:00 | . 2 | 5.291224 | 5.083363 | 00:00 | . 3 | 4.361655 | 3.622583 | 00:00 | . 4 | 3.702361 | 3.748363 | 00:00 | . 5 | 3.360657 | 10.190661 | 00:00 | . 6 | 3.210521 | 0.754187 | 00:00 | . 7 | 3.172959 | 5.972820 | 00:00 | . 8 | 3.032082 | 1.291983 | 00:00 | . 9 | 2.956714 | 3.510024 | 00:00 | . 10 | 2.974406 | 1.764339 | 00:00 | . 11 | 2.981836 | 1.889399 | 00:00 | . 12 | 2.867945 | 4.597968 | 00:00 | . 13 | 2.763726 | 2.212024 | 00:00 | . 14 | 2.598257 | 1.323128 | 00:00 | . 15 | 2.471248 | 1.911128 | 00:00 | . 16 | 2.358785 | 3.345455 | 00:00 | . 17 | 2.238375 | 3.601553 | 00:00 | . 18 | 2.120937 | 3.233053 | 00:00 | . 19 | 2.058635 | 2.512881 | 00:00 | . 20 | 1.991236 | 1.336259 | 00:00 | . 21 | 1.916328 | 0.684032 | 00:00 | . 22 | 1.861911 | 0.610294 | 00:00 | . 23 | 1.806844 | 0.609783 | 00:00 | . 24 | 1.761676 | 0.634143 | 00:00 | . 25 | 1.706081 | 0.676253 | 00:00 | . 26 | 1.670432 | 0.692672 | 00:00 | . 27 | 1.620014 | 0.710688 | 00:00 | . 28 | 1.588594 | 0.721118 | 00:00 | . 29 | 1.553502 | 0.685278 | 00:00 | . &lt;matplotlib.legend.Legend at 0x7f66d33b43c8&gt; . We can see clearly from the above figure that the neural network is giving good results, and far better than decision trees and RF, on extrapolation. This is because a neural network could fit a complex non-linear function easily, and could generalize better than on unseen data. Now let&#39;s see the neural network performance on the Kaggle dataset. . df = pd.read_csv(&#39;/home/nitish/Downloads/bluebook-bulldozers/TrainAndValid.csv&#39;, low_memory=False) sizes = (&#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39;) df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) dep_var = &#39;SalePrice&#39; df[dep_var] = np.log(df[dep_var]) df = add_datepart(df, &#39;saledate&#39;) procs = [Categorify, FillMissing] cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) cont,cat = cont_cat_split(df, 1, dep_var=dep_var) to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y xs.loc[xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 valid_xs.loc[valid_xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 m = DecisionTreeRegressor() m.fit(xs, y); def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) m = rf(xs, y) def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(m, xs) fi[:10] to_keep = fi[fi.imp&gt;0.005].cols xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) df_dom = pd.concat([xs_final, valid_xs_final]) is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final)) m = rf(df_dom, is_valid) rf_feat_importance(m, df_dom)[:6] m = rf(xs_final, y) # print(&#39;orig&#39;, m_rmse(m, valid_xs_final, valid_y)) time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) ################ NN df_nn = pd.read_csv(&#39;/home/nitish/Downloads/bluebook-bulldozers/TrainAndValid.csv&#39;, low_memory=False) df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) df_nn[dep_var] = np.log(df_nn[dep_var]) df_nn = add_datepart(df_nn, &#39;saledate&#39;) df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] cont_nn,cat_nn = cont_cat_split(df_nn_final, dep_var=dep_var) cont_nn.append(&#39;saleElapsed&#39;) cat_nn.remove(&#39;saleElapsed&#39;) cat_nn.remove(&#39;fiModelDescriptor&#39;) procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final.astype({&#39;saleElapsed&#39;: &#39;int64&#39;}), procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) dls = to_nn.dataloaders(1024) from fastai.tabular.all import * learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) learn.fit_one_cycle(5, 1e-2) preds,targs = learn.get_preds() r_mse(preds,targs) . orig 0.230883 . epoch train_loss valid_loss time . 0 | 0.084567 | 0.081409 | 00:40 | . 1 | 0.071803 | 0.077324 | 00:45 | . 2 | 0.063720 | 0.067529 | 00:39 | . 3 | 0.057461 | 0.064616 | 00:35 | . 4 | 0.054729 | 0.064906 | 00:35 | . 0.254766 . We are getting rmse of 0.25 here, and each epoch takes ~40 seconds, which is on CPU, on GPU it will take around 1 2 seconds. If we train for 15 epochs total we could achieve ~0.23 rmse, same as rf. I didn&#39;t train for 15 more epochs because while writing the blog I was on CPU ;-) . Categorical Embeddings . If we look at the architecture of our model, we can see that before the first layer there is an embedding layer. It&#39;s for categorical variables. So rather than keeping the categorical variables category-values in the df and use one-hot encoding, NN should use embeddings instead because of two following 2 main reasons: . One-Hot encoding is expensive both space-wise and computationally. | We can use the embeddings we get to get the idea about the relationship of different categories. Reason for 1 is that if a categorical column has 5000 distinct categories, and there are a total of 1000 training examples, then it will use $5$x$10^6$ length array just for feeding the input to the NN, which would be infeasible if these numbers are bigger. | Reason for 2 is suppose we get embeddings for words then basically we are getting a fixed dimensional representation of each of the word, and we can do various mathematical operations on the word-embeddings. E.g., we can find the distance between any two words which could be interpreted as how similar or distinct the words are, cluster the similar words together, etc. . In Fastai, the target dimension of an embedding is found out by the learner itself during training. So, there is a total of 10 categorical variables, so 11 embedding matrices are shown below, where for example the third embedding matrix is of shape (75, 18), meaning it has 75 distinct categories and it&#39;s calculating an 18-dim vector for each of the category. . learn.model . TabularModel( (embeds): ModuleList( (0): Embedding(7, 5) (1): Embedding(3, 3) (2): Embedding(75, 18) (3): Embedding(4, 3) (4): Embedding(178, 29) (5): Embedding(5060, 190) (6): Embedding(7, 5) (7): Embedding(13, 7) (8): Embedding(7, 5) (9): Embedding(18, 8) ) (emb_drop): Dropout(p=0.0, inplace=False) (bn_cont): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (layers): Sequential( (0): LinBnDrop( (0): BatchNorm1d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Linear(in_features=276, out_features=500, bias=False) (2): ReLU(inplace=True) ) (1): LinBnDrop( (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Linear(in_features=500, out_features=250, bias=False) (2): ReLU(inplace=True) ) (2): LinBnDrop( (0): Linear(in_features=250, out_features=1, bias=True) ) (3): SigmoidRange(low=8, high=12) ) ) . Using Embeddings in a Random Forest . There is a paper, &quot;Entity Embeddings of Categorical Variables&quot; which tells about embeddings of categorical variables. The abstract of the paper is: . Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables... [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit... As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering. . We can use the embeddings we got for different categorical variables as features and train a random forest on this new data. It can be seen that RF with embeddings instead of raw categories performs better. I have tried using NN embeddings and trained an RF on top of it, but get only slightly better results (might be some bug/issue with my code). We can get the embedding matrix like: . cat_nn[0],list(learn.model.embeds.parameters())[0] . (&#39;ProductSize&#39;, Parameter containing: tensor([[-0.0970, -0.0665, 0.1017, 0.0527, -0.2218], [-0.1284, 0.0566, -0.1521, -0.0613, 0.0242], [ 0.2466, 0.1294, -0.0728, -0.2354, 0.1425], [ 0.0848, 0.1131, -0.1244, -0.0624, 0.1196], [ 0.0515, 0.0218, -0.0528, -0.0112, -0.0019], [-0.2016, -0.3449, 0.2277, 0.2383, -0.1808], [-0.0733, -0.1717, 0.2554, 0.1517, -0.1284]], requires_grad=True)) . This is the 7x5 embedding matrix for the categorical variable ProductSize. There are 6 distinct categories in this feature, and one category (the last one) is na, for missing ones or not applicable. . cat=[] emb=[] for idx, item in enumerate(list(learn.model.embeds.parameters())): cat.append(cat_nn[idx]) emb.append(item.shape) df_ = pd.DataFrame() df_[&#39;categories&#39;]=cat df_[&#39;embeddings&#39;]=emb . df_ . categories embeddings . 0 ProductSize | (7, 5) | . 1 Coupler_System | (3, 3) | . 2 fiProductClassDesc | (75, 18) | . 3 Hydraulics_Flow | (4, 3) | . 4 fiSecondaryDesc | (178, 29) | . 5 fiModelDesc | (5060, 190) | . 6 Enclosure | (7, 5) | . 7 Hydraulics | (13, 7) | . 8 ProductGroup | (7, 5) | . 9 Tire_Size | (18, 8) | . Conclusion . We have covered the basic ML techniques, a NN network, and using embeddings obtained from a NN in an RF. We have seen better extrapolation by a NN than RF. We have trained a simple, not so deep NN on the Kaggle dataset which gives the same rmse or a little better than RF. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/11/24/DLforCNN.html",
            "relUrl": "/2020/11/24/DLforCNN.html",
            "date": " • Nov 24, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "ML/DL techniques for Tabular Modeling PART II",
            "content": "Introduction . Hello everyone, in PART I we have read and implemented a decision tree on a very simple dataset, and have seen it&#39;s poor extrapolation power. Now we will look into a much better model than the decision tree but fundamentally similar, known as Random Forest (RF). But before RF let&#39;s look into bagging. . Bagging . We have seen in PART I that a decision tree model has high variance and low bias, and by high variance, it means the variance of the predictions given by the model. In order to reduce this variance, multiple decision trees on a randomly sampled subset of the training dataset are modeled, and predictions are based on the average of predictions of all the decision trees. The algorithm is as follows: . Randomly sample some rows from the training dataset, these are known as bootstrap replicas. | Implement and train a decision tree from the dataset taken in (1). | Save the model and repeat steps 1, 2 multiple times. | At the time of prediction, take the average of predictions of all the decision trees. | This is beneficial because now the error will also average out which will lead to lesser variance. Mathematically, If we draw n independent samples $(X_1, X_2,..., X_n)$ where $X_i in N(0, 1)$, then $ Sigma_{i=1}^n X_i in N(0, frac{1}{n})$. This is the basic idea behind bagging. If we train multiple decision trees on an independently drawn training set then averaging out their predictions will give the same mean but lesser variance. . The independence of training sets is a very important condition otherwise the variance won&#39;t be any lesser. For example, if we have trained multiple decision trees on the same set of data then more or less all the decision trees will have the same shape and will give similar predictions, or highly correlated predictions. Therefore averaging won&#39;t be beneficial. . Random Forests . In RF, we not only sample random rows of data like bagging but also sample the columns to be included. Then train decision tree on this randomly sampled dataset. Repeat the same procedure and create multiple decision trees. In comparison with the previous approach, Bagging is choosing random rows to make models, and taking average at the end, whereas in RF we randomly choosing rows as well as columns to train a decision tree. We train multiple decision trees on different sampled data and take average of them during prediction. . If we model the decision tree on the Kaggle dataset, then it gives the following result: . m = DecisionTreeRegressor(max_leaf_nodes=10000) m.fit(xs, y); print(f&#39;Training error {m_rmse(m, xs, y)}, Validation Error{m_rmse(m, valid_xs, valid_y)}&#39;) . Training error 0.191348, Validation Error0.280134 . Whereas the RF gives far better training loss, validation loss, and less overfitting. . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) m = rf(xs, y) print(f&#39;Training error {m_rmse(m, xs, y)}, Validation Error{m_rmse(m, valid_xs, valid_y)}&#39;) . Training error 0.171021, Validation Error0.234668 . Here I have passed 40 decision trees (n_estimators), with max_samples 200k, and max features are half of the total features available. To see how RF is better than decision trees, we need to get validation loss values for different n_estimators values. . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . fig, ax = plt.subplots(figsize=(10,6)) ax.plot( range(40), [r_mse(preds[:i+1].mean(axis=0), valid_y) for i in range(40)]) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(5)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;n_estimators&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;Validation Loss&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) . Text(0, 0.5, &#39;Validation Loss&#39;) . It could be clearly seen as the number of estimators increase there is a decrease in validation loss, which gets stable for n_estimators &gt; 30. . Out-Of-Bag Error (OOB) . We know that in a decision tree in RF we sample rows from the training set. The rows which are not selected for a decision tree could be thought of as a validation dataset for that decision tree model because that data was not used during training. Computing loss on such rows that were not seen during the training is known as OOB error. The benefit of this approach is we do not need to partition the whole data into 2 subsets of training and validation dataset. This is particularly useful in case of less data is available. . If say row-i is not included in 10 decision trees out of 40, then we will pass row-i to those 10 decision trees and take the average to get OOB prediction. Similar is done for all the rows in the dataset. . r_mse(m.oob_prediction_, y) . 0.210836 . OOB loss is 0.21 whereas validation loss was 0.23, which hints that the validation loss is not only because of a weaker model, but data patterns might have been slightly different, or something else is causing the error in the validation dataset. . Model Refinements . Prediction Confidence . When a model makes a prediction, we want to know how confident the model is in the prediction. It is important because we will use that prediction only when the model has a certain degree of confidence in the prediction. . We know in RF there are multiple decision tree models, all of which make predictions and an average is taken out which is the final prediction of the RF model. So, if there are n decision trees, we will have n predictions, we could then take variance or standard deviation (std) of these n predictions. It tells us how sure or unsure the RF is because if the std is high, then all the models are not making very similar predictions, and vice versa. The benefit of this is we could be more cautious while use predictions that have high std. . Feature Importance . If we want to know which feature is the most important in our decision tree, or whether feature a is more important or not than the feature b. The answer to this question could be find out from the decision tree. In a decision tree, a split is used to partition the root dataset into two disjoint subsets. If we look at all the splits and for each split calculate the improvement in the rmse loss value weighted by the number of data samples in each new branch, then the calculated value will tell how much this feature has improved the decision tree in making the correct predictions. For all the features we can calculate in this way and sum across all the decision trees in the RF, and finally normalize it so that the sum=1. . The motivation for the feature importance formula is that we want 2 things to happen at every split: . We want the difference in mse to be high. | We want no. of rows in the new partition to be more because this tells good generalization. | E.g., in our dataset, we could calculate feature importance as: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) fi = rf_feat_importance(m, xs) plot_fi(fi[:30]); . Above tells the relative feature importance of different features in our dataset. . Now once we have the feature importance of all the features calculated for our model, we can use this information to eliminate the features which are very less important, i.e. have feature importance value less than some threshold. . to_keep = fi[fi.imp&gt;0.005].cols print(f&#39;We have reduced number of features from {df.shape[1]} to {len(to_keep)}&#39;) . We have reduced number of features from 65 to 20 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] m = rf(xs_imp, y) print(f&#39;Training error {m_rmse(m, xs_imp, y)}, Validation Error{m_rmse(m, valid_xs_imp, valid_y)}&#39;) . Training error 0.181062, Validation Error0.231971 . We can see there isn&#39;t much change in the losses, but this has made our model quite simple with lesser data, $ frac{1}{3}^{rd}$ the original features. . Redundant Features . We can calculate the Spearman Rank Correlation Coefficient for all the pair of columns, and if some columns have high correlation then we can take only one of them, and remove the other redundant feature. We can try experimenting with a different set of redundant features removed and chose the model with the least validation loss. . cluster_columns(xs_imp) . Those columns which are joined together in the left region have high correlation among them than the columns which are joined in the right. Therefore, &#39;saleYear&#39; and &#39;saleElapsed&#39; have a high correlation. &#39;ProductGroup&#39; and &#39;ProductGroupDesc&#39; also have a high correlation. We can try removing one of these pairs of columns. . to_drop=[&#39;saleYear&#39;, &#39;Grouser_Tracks&#39;, &#39;ProductGroup&#39;, &#39;fiBaseModel&#39;] . xs_imp_nr = xs_imp.drop(to_drop, axis=1) valid_xs_imp_nr = valid_xs_imp.drop(to_drop, axis=1) m = rf(xs_imp_nr, y) print(f&#39;Training error {m_rmse(m, xs_imp_nr, y)}, Validation Error{m_rmse(m, valid_xs_imp_nr, valid_y)}&#39;) . Training error 0.182557, Validation Error0.232808 . print(f&#39;We have reduced number of features from {xs_imp.shape[1]} to {xs_imp_nr.shape[1]}&#39;) . We have reduced number of features from 20 to 16 . This has further reduced our number of features from 20 to 16, which will in turn make our model simpler, easy to debug, and more understandable. Improvements we have done so far have reduced the features and this is more or less the best we could do with RF. . Relationship Between a Feature and Dependent Variable . Suppose we want to inspect how a feature impacts the dependent variable. E.g., we want to enquire about the relationship between &#39;YearMade&#39; and &#39;salePrice&#39;. One way could be we could simply find the average value of salePrice for every YearMade and plot the values, but that would be wrong. The reason being some of the other features could be impacting salePrice in some years but not in the other years. . As mentioned in fastai book, &quot;Merely averaging over all the auctions that have the same YearMade would also capture the effect of how every other field also changed along with YearMade and how that overall change affected price.&quot; . The simple solution to find out a relationship is to replace all the YearMade in the dataset with one value, and then make predictions and take the average of those predictions. Similarly, replace one by one all the years, and so we will get average salePrice value predictions for all the years. . yrs = xs_imp_nr.YearMade.unique() yrs.sort() yrs . array([1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2014]) . We have the above mentioned values for YearMade ranging from 1950 to 2014. We can replace all the YearMade values in the training dataset with 1950, and take the average of predictions, then replace with 1951 and take the average of predictions, similarly repeating for all the years in the domain of YearMade. The implementation is shown below. . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(8, 4)) plot_partial_dependence(m, xs_imp_nr[:10000], [&#39;YearMade&#39;], grid_resolution=20, ax=ax); . We can clearly see the salePrice is increasing almost linearly from 1963-64 to 2014. We are calculating the log of salePrice, therefore in fact the increase is exponential from 1963-64 to 2014 in salePrice value, which makes sense. . Extrapolation Problem . As we did in PART I, we will construct a dataset and train an RF on it. Then will try to extrapolate on the out-of-domain data. . x = np.linspace(0, 10, 110) y = x + np.random.randn(110) tr_x, tr_y = x[:80], y[:80] val_x, val_y = x[80:], y[80:] tr_x = tr_x.reshape((80,1)) val_x = val_x.reshape((30,1)) . m = RandomForestRegressor() m.fit(tr_x, tr_y); fig, ax = plt.subplots(figsize=(16,8)) ax.scatter(x,y, marker=&#39;+&#39;, label=&#39;actual data&#39;) ax.scatter(tr_x, m.predict(tr_x), label=&#39;predicted data on training dataset&#39;) ax.scatter(val_x, m.predict(val_x), label=&#39;predicted data on validation dataset&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;y&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:15}) . &lt;matplotlib.legend.Legend at 0x7f26ac732588&gt; . We could see that RF has done better than a decision tree on the training set, and also it would have done better on in-domain new data. But for the out-of-domain dataset, suffers from the same problem as a decision tree. In the next part, we will look into NN and see what it has to offer on extrapolation. . Conclusion . In PART II, we have covered Random Forests, done many improvements on it, and made training/validation losses lesser. In the next part, we will implement a Neural Network and then see whether it could overcome the RF shortcomings or not. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/11/17/DLforCNN.html",
            "relUrl": "/2020/11/17/DLforCNN.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "ML/DL techniques for Tabular Modeling PART I",
            "content": "Introduction . Tabular Modelling takes data in the form of a table, where generally we want to learn about a column&#39;s value from all the other columns&#39; values. The column we want to learn is known as a dependent variable and others are known as independent variables. The learning could be both like a classification problem or regression problem. We will look into various machine learning models such as decision trees, random forests, etc, also we&#39;ll look for what deep learning has to offer in tabular modeling. . Dataset . I will be using Kaggle competition dataset on all the models so that it will be easier to understand and compare different models. I have loaded it into a dataframe df. . df.head() . SalesID SalePrice MachineID ModelID ... Blade_Type Travel_Controls Differential_Type Steering_Controls . 0 1139246 | 66000.0 | 999089 | 3157 | ... | NaN | NaN | Standard | Conventional | . 1 1139248 | 57000.0 | 117657 | 77 | ... | NaN | NaN | Standard | Conventional | . 2 1139249 | 10000.0 | 434808 | 7009 | ... | NaN | NaN | NaN | NaN | . 3 1139251 | 38500.0 | 1026470 | 332 | ... | NaN | NaN | NaN | NaN | . 4 1139253 | 11000.0 | 1057373 | 17311 | ... | NaN | NaN | NaN | NaN | . 5 rows × 53 columns . The key fields are in train.csv are: . SalesID: the unique identifier of the sale | MachineID: the unique identifier of a machine. A machine can be sold multiple times | saleprice: what the machine sold for at auction (only provided in train.csv) | saledate: the date of the sale | . For this competition, we need to predict the log of the sale price of bulldozers sold at auctions. We will try to build different ML and DL models which will be predicting $log$(sale price). . Decision Trees . A decision tree makes a split in data based on the values of a column. For example, suppose we have data for different persons for their age, whether they eat healthy, whether they exercise, etc, and want to predict whether they are fit or unfit based on the data then we can use the following decision tree. . . At each level, the data is divided into 2 groups for the next level, e.g. at the first level, whether age&lt;30 or not divides the whole dataset into 2 smaller datasets, and similary the data is split again until we reach leaf node of 2 classes: FIT or UNFIT. . In the real world, data is way more complex containing a lot of columns. E.g., in our dataframe df, there are 53 columns. So the question arises which column to chose for each split and what should be the value at which it is split. The answer is to try for every column and each value present in a column for the split. So if there are n columns and each column have x different values then we need to try n*x splits and chose the best one on some criteria. When trying a split, then whole data will be divided into 2 groups for that level, so we can take the average of the sale price of a group as the predicted sale price for all the rows in that group, and can calculate rmse distance between predictions and actual sale price. This will give us a number, which is our loss value, if bigger tells our predictions are far from the actual sale price and vice-versa. So the algorithm for building a decision tree could be written as: . Loop through all the columns in the training dataset. | Loop through all the possible values for a column. If the column contains categorical data then chose the condition as &quot;equal to&quot; a category and &quot;not equal to&quot; a category. If the column contains continuous data then for all the distinct values split on &quot;less than equal to&quot; and &quot;greater than&quot; the value. | Find the average sale price for each of the groups, this is our prediction. Calculate rmse from the actual values of the saleprice. | The rmse of a split could be set as the sum of rmse for all groups after the split. | After looping through all the columns and all possible splits for each column chose the split with the least rmse. | Continue the same process recursively on the child groups until some stopping criteria are reached like maximum number of the leaf nodes, minimum number of data items per group, etc. | Below is given an example of a decision tree. In the root node, the value is simply the average of all the training dataset which would be the most simple prediction we could calculate for a new datapoint is to simply give a prediction of 10.1 every time. Mean Square Error (mse) is 0.48, and there is a total of 404710 samples, which is actually the total number of samples in the training dataset. . Now for the split, it has tried all the columns and all the possible values for a column, and it came with $Coupler _System leq 0.5$ split. This would split the whole dataset into two smaller datasets. When the condition is True it resulted in 360847 samples, with mse of 0.42 and an average value of sale price as 10.21. When the condition is False it resulted in 43863 samples, with mse of 0.12 and an average sale price value of 9.21. It could be seen that this split has improved our prediction, and our model has learnt some pattern because now the weighted average mse is (360847 0.42 + 43863 0.12)/(404710) = 0.38 &lt; 0.48. . Similarly, splitting the &quot;True condition child&quot; on $YearMade leq 0.42$ further decreases the mse, which means our predictions are further closer to the actual values . . Overfitting and Underfitting in decision trees . Underfitting in the decision trees will be when we make very few splits, or no splits at all, e.g., in the root node the average value is 10.1, and if we use this value as prediction, then it&#39;s clearly a naive solution to a complex problem, which is therefore an underfitting. This is the case of high bias and low variance. . Overfitting will be when there are way too many splits such that in extreme case there is one training sample per leaf node, which is actually the model has memorized the training dataset. It is overfitting because although the mse will be 0 for the training dataset, it will be very high for the validation dataset, as the model will fail to generalize on unseen datapoints. This is the case of low bias and high variance. . Data Generation step: . x = np.linspace(0, 10, 110) y = x + np.random.randn(110) my_list = [0]*30 + [1]*80 random.shuffle(my_list) my_list = [True if i==1 else False for i in my_list] tr_x, tr_y = x[np.where(my_list)[0]],y[np.where(my_list)[0]] my_list = [not elem for elem in my_list] val_x, val_y = x[np.where(my_list)[0]],y[np.where(my_list)[0]] tr_x = tr_x.reshape(tr_x.shape[0],1) val_x = val_x.reshape(val_x.shape[0],1) . Underfitting Case . In the underfitting case, I have set max_leaf_nodes=2, so that bias will be high. . m = DecisionTreeRegressor(max_leaf_nodes=2) m.fit(tr_x, tr_y); fig, ax = plt.subplots(figsize=(16,8)) ax.scatter(x,y, marker=&#39;+&#39;, label=&#39;actual data&#39;) ax.scatter(tr_x, m.predict(tr_x), label=&#39;predicted data on training dataset&#39;) ax.scatter(val_x, m.predict(val_x), label=&#39;predicted data on validation dataset&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;y&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:15}) . &lt;matplotlib.legend.Legend at 0x7f9bb1f3f240&gt; . In the above example, I have generated a dataset with $x = y + epsilon$, where $ epsilon in N(0,1)$ is the random noise. I have generated this data because it&#39;s 2-d data, much simpler, and easy to visualize than the complex Kaggle dataset. . A decision tree is implemented which tries to learn the relationship between x and y and predicts y from x. Training data is randomly chosen 80 samples from 110 samples, and the remaining 30 are in validation data. Stopping criteria is set as the max number of leaf nodes = 10. In the above figure, the orange ones are training samples and the green ones are validation samples. . print(f&#39;Training rmse is {m_rmse(m, tr_x, tr_y)}, and validation rmse is {m_rmse(m, val_x, val_y)}&#39;) draw_tree(m, pd.DataFrame([tr_x,tr_y], columns=[&#39;tr_y&#39;])) . Training rmse is 1.6552681120862984, and validation rmse is 1.787770216476971 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 tr_y ≤ 5.0 mse = 10.0 samples = 80 value = 5.0 1 mse = 2.0 samples = 36 value = 3.0 0&#45;&gt;1 True 2 mse = 3.0 samples = 44 value = 8.0 0&#45;&gt;2 False Overfitting Case . In the overfitting case, I have set max_leaf_nodes=100. This leads to a huge decision tree with each leaf node containing one training example only. Therefore, bias will be zero, the variance will be high and there will be overfitting. . m = DecisionTreeRegressor(max_leaf_nodes=100) m.fit(tr_x, tr_y); fig, ax = plt.subplots(figsize=(16,8)) ax.scatter(x,y, marker=&#39;+&#39;, label=&#39;actual data&#39;) ax.scatter(tr_x, m.predict(tr_x), label=&#39;predicted data on training dataset&#39;) ax.scatter(val_x, m.predict(val_x), label=&#39;predicted data on validation dataset&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;y&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:15}) . &lt;matplotlib.legend.Legend at 0x7f9bb1eb8d68&gt; . print(f&#39;Training rmse is {m_rmse(m, tr_x, tr_y)}, and validation rmse is {m_rmse(m, val_x, val_y)}&#39;) draw_tree(m, pd.DataFrame([tr_x,tr_y], columns=[&#39;tr_y&#39;])) . Training rmse is 0.0, and validation rmse is 1.2925307400522905 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 tr_y ≤ 5.0 mse = 10.0 samples = 80 value = 5.0 1 tr_y ≤ 3.0 mse = 2.0 samples = 36 value = 3.0 0&#45;&gt;1 True 2 tr_y ≤ 7.0 mse = 3.0 samples = 44 value = 8.0 0&#45;&gt;2 False 5 tr_y ≤ 1.0 mse = 1.0 samples = 21 value = 1.0 1&#45;&gt;5 6 tr_y ≤ 4.0 mse = 1.0 samples = 15 value = 4.0 1&#45;&gt;6 9 tr_y ≤ 0.0 mse = 1.0 samples = 11 value = 1.0 5&#45;&gt;9 10 tr_y ≤ 3.0 mse = 0.0 samples = 10 value = 2.0 5&#45;&gt;10 15 tr_y ≤ 0.0 mse = 0.0 samples = 2 value = 2.0 9&#45;&gt;15 16 tr_y ≤ 0.0 mse = 0.0 samples = 9 value = 1.0 9&#45;&gt;16 49 mse = 0.0 samples = 1 value = 1.0 15&#45;&gt;49 50 mse = 0.0 samples = 1 value = 3.0 15&#45;&gt;50 59 tr_y ≤ 0.0 mse = 0.0 samples = 2 value = 0.0 16&#45;&gt;59 60 tr_y ≤ 1.0 mse = 0.0 samples = 7 value = 1.0 16&#45;&gt;60 147 mse = 0.0 samples = 1 value = 0.0 59&#45;&gt;147 148 mse = &#45;0.0 samples = 1 value = 0.0 59&#45;&gt;148 61 tr_y ≤ 1.0 mse = 0.0 samples = 3 value = 1.0 60&#45;&gt;61 62 tr_y ≤ 1.0 mse = 0.0 samples = 4 value = 0.0 60&#45;&gt;62 115 tr_y ≤ 1.0 mse = 0.0 samples = 2 value = 1.0 61&#45;&gt;115 116 mse = 0.0 samples = 1 value = 2.0 61&#45;&gt;116 149 mse = 0.0 samples = 1 value = 1.0 115&#45;&gt;149 150 mse = 0.0 samples = 1 value = 1.0 115&#45;&gt;150 63 tr_y ≤ 1.0 mse = 0.0 samples = 3 value = 0.0 62&#45;&gt;63 64 mse = 0.0 samples = 1 value = 1.0 62&#45;&gt;64 127 tr_y ≤ 1.0 mse = 0.0 samples = 2 value = 0.0 63&#45;&gt;127 128 mse = 0.0 samples = 1 value = &#45;0.0 63&#45;&gt;128 133 mse = 0.0 samples = 1 value = 0.0 127&#45;&gt;133 134 mse = 0.0 samples = 1 value = 0.0 127&#45;&gt;134 41 tr_y ≤ 1.0 mse = 0.0 samples = 7 value = 2.0 10&#45;&gt;41 42 tr_y ≤ 3.0 mse = 0.0 samples = 3 value = 3.0 10&#45;&gt;42 69 mse = 0.0 samples = 1 value = 2.0 41&#45;&gt;69 70 tr_y ≤ 2.0 mse = 0.0 samples = 6 value = 2.0 41&#45;&gt;70 71 tr_y ≤ 2.0 mse = 0.0 samples = 3 value = 1.0 70&#45;&gt;71 72 tr_y ≤ 2.0 mse = 0.0 samples = 3 value = 2.0 70&#45;&gt;72 83 mse = 0.0 samples = 1 value = 2.0 71&#45;&gt;83 84 tr_y ≤ 2.0 mse = 0.0 samples = 2 value = 1.0 71&#45;&gt;84 135 mse = 0.0 samples = 1 value = 1.0 84&#45;&gt;135 136 mse = 0.0 samples = 1 value = 1.0 84&#45;&gt;136 105 mse = 0.0 samples = 1 value = 2.0 72&#45;&gt;105 106 tr_y ≤ 2.0 mse = 0.0 samples = 2 value = 2.0 72&#45;&gt;106 141 mse = 0.0 samples = 1 value = 2.0 106&#45;&gt;141 142 mse = 0.0 samples = 1 value = 2.0 106&#45;&gt;142 137 mse = 0.0 samples = 1 value = 3.0 42&#45;&gt;137 138 tr_y ≤ 3.0 mse = 0.0 samples = 2 value = 3.0 42&#45;&gt;138 139 mse = 0.0 samples = 1 value = 2.0 138&#45;&gt;139 140 mse = 0.0 samples = 1 value = 3.0 138&#45;&gt;140 13 tr_y ≤ 4.0 mse = 0.0 samples = 6 value = 3.0 6&#45;&gt;13 14 tr_y ≤ 4.0 mse = 0.0 samples = 9 value = 5.0 6&#45;&gt;14 51 tr_y ≤ 3.0 mse = 0.0 samples = 5 value = 3.0 13&#45;&gt;51 52 mse = &#45;0.0 samples = 1 value = 2.0 13&#45;&gt;52 55 tr_y ≤ 3.0 mse = 0.0 samples = 2 value = 3.0 51&#45;&gt;55 56 tr_y ≤ 3.0 mse = 0.0 samples = 3 value = 4.0 51&#45;&gt;56 103 mse = 0.0 samples = 1 value = 3.0 55&#45;&gt;103 104 mse = 0.0 samples = 1 value = 3.0 55&#45;&gt;104 107 tr_y ≤ 3.0 mse = 0.0 samples = 2 value = 4.0 56&#45;&gt;107 108 mse = &#45;0.0 samples = 1 value = 4.0 56&#45;&gt;108 109 mse = 0.0 samples = 1 value = 4.0 107&#45;&gt;109 110 mse = 0.0 samples = 1 value = 3.0 107&#45;&gt;110 25 mse = 0.0 samples = 1 value = 6.0 14&#45;&gt;25 26 tr_y ≤ 5.0 mse = 0.0 samples = 8 value = 4.0 14&#45;&gt;26 57 tr_y ≤ 4.0 mse = 0.0 samples = 7 value = 4.0 26&#45;&gt;57 58 mse = &#45;0.0 samples = 1 value = 4.0 26&#45;&gt;58 65 tr_y ≤ 4.0 mse = 0.0 samples = 3 value = 4.0 57&#45;&gt;65 66 tr_y ≤ 5.0 mse = 0.0 samples = 4 value = 5.0 57&#45;&gt;66 81 tr_y ≤ 4.0 mse = 0.0 samples = 2 value = 4.0 65&#45;&gt;81 82 mse = 0.0 samples = 1 value = 4.0 65&#45;&gt;82 153 mse = 0.0 samples = 1 value = 4.0 81&#45;&gt;153 154 mse = 0.0 samples = 1 value = 4.0 81&#45;&gt;154 79 tr_y ≤ 4.0 mse = 0.0 samples = 3 value = 5.0 66&#45;&gt;79 80 mse = &#45;0.0 samples = 1 value = 4.0 66&#45;&gt;80 89 tr_y ≤ 4.0 mse = 0.0 samples = 2 value = 5.0 79&#45;&gt;89 90 mse = 0.0 samples = 1 value = 5.0 79&#45;&gt;90 121 mse = 0.0 samples = 1 value = 5.0 89&#45;&gt;121 122 mse = 0.0 samples = 1 value = 5.0 89&#45;&gt;122 3 tr_y ≤ 6.0 mse = 0.0 samples = 13 value = 6.0 2&#45;&gt;3 4 tr_y ≤ 8.0 mse = 2.0 samples = 31 value = 9.0 2&#45;&gt;4 17 tr_y ≤ 5.0 mse = 0.0 samples = 9 value = 5.0 3&#45;&gt;17 18 tr_y ≤ 6.0 mse = 0.0 samples = 4 value = 6.0 3&#45;&gt;18 45 tr_y ≤ 5.0 mse = 0.0 samples = 3 value = 6.0 17&#45;&gt;45 46 tr_y ≤ 6.0 mse = 0.0 samples = 6 value = 5.0 17&#45;&gt;46 91 mse = 0.0 samples = 1 value = 6.0 45&#45;&gt;91 92 tr_y ≤ 5.0 mse = 0.0 samples = 2 value = 6.0 45&#45;&gt;92 99 mse = 0.0 samples = 1 value = 5.0 92&#45;&gt;99 100 mse = 0.0 samples = 1 value = 6.0 92&#45;&gt;100 47 tr_y ≤ 5.0 mse = 0.0 samples = 3 value = 5.0 46&#45;&gt;47 48 tr_y ≤ 6.0 mse = 0.0 samples = 3 value = 6.0 46&#45;&gt;48 117 mse = 0.0 samples = 1 value = 5.0 47&#45;&gt;117 118 tr_y ≤ 6.0 mse = 0.0 samples = 2 value = 5.0 47&#45;&gt;118 119 mse = 0.0 samples = 1 value = 4.0 118&#45;&gt;119 120 mse = &#45;0.0 samples = 1 value = 5.0 118&#45;&gt;120 143 mse = 0.0 samples = 1 value = 6.0 48&#45;&gt;143 144 tr_y ≤ 6.0 mse = 0.0 samples = 2 value = 6.0 48&#45;&gt;144 151 mse = 0.0 samples = 1 value = 6.0 144&#45;&gt;151 152 mse = &#45;0.0 samples = 1 value = 6.0 144&#45;&gt;152 67 tr_y ≤ 6.0 mse = 0.0 samples = 3 value = 6.0 18&#45;&gt;67 68 mse = 0.0 samples = 1 value = 7.0 18&#45;&gt;68 129 tr_y ≤ 6.0 mse = 0.0 samples = 2 value = 6.0 67&#45;&gt;129 130 mse = &#45;0.0 samples = 1 value = 6.0 67&#45;&gt;130 131 mse = 0.0 samples = 1 value = 6.0 129&#45;&gt;131 132 mse = 0.0 samples = 1 value = 6.0 129&#45;&gt;132 7 tr_y ≤ 7.0 mse = 1.0 samples = 11 value = 8.0 4&#45;&gt;7 8 tr_y ≤ 10.0 mse = 1.0 samples = 20 value = 9.0 4&#45;&gt;8 19 tr_y ≤ 7.0 mse = 1.0 samples = 4 value = 8.0 7&#45;&gt;19 20 tr_y ≤ 7.0 mse = 1.0 samples = 7 value = 7.0 7&#45;&gt;20 43 tr_y ≤ 7.0 mse = 0.0 samples = 3 value = 8.0 19&#45;&gt;43 44 mse = 0.0 samples = 1 value = 9.0 19&#45;&gt;44 75 tr_y ≤ 7.0 mse = 0.0 samples = 2 value = 8.0 43&#45;&gt;75 76 mse = 0.0 samples = 1 value = 7.0 43&#45;&gt;76 77 mse = 0.0 samples = 1 value = 8.0 75&#45;&gt;77 78 mse = 0.0 samples = 1 value = 9.0 75&#45;&gt;78 27 mse = 0.0 samples = 1 value = 6.0 20&#45;&gt;27 28 tr_y ≤ 7.0 mse = 1.0 samples = 6 value = 8.0 20&#45;&gt;28 29 mse = 0.0 samples = 1 value = 9.0 28&#45;&gt;29 30 tr_y ≤ 7.0 mse = 0.0 samples = 5 value = 7.0 28&#45;&gt;30 101 mse = 0.0 samples = 1 value = 7.0 30&#45;&gt;101 102 tr_y ≤ 8.0 mse = 0.0 samples = 4 value = 7.0 30&#45;&gt;102 123 tr_y ≤ 8.0 mse = 0.0 samples = 3 value = 7.0 102&#45;&gt;123 124 mse = 0.0 samples = 1 value = 7.0 102&#45;&gt;124 125 tr_y ≤ 7.0 mse = 0.0 samples = 2 value = 7.0 123&#45;&gt;125 126 mse = &#45;0.0 samples = 1 value = 8.0 123&#45;&gt;126 157 mse = 0.0 samples = 1 value = 7.0 125&#45;&gt;157 158 mse = 0.0 samples = 1 value = 7.0 125&#45;&gt;158 11 tr_y ≤ 9.0 mse = 1.0 samples = 19 value = 9.0 8&#45;&gt;11 12 mse = &#45;0.0 samples = 1 value = 11.0 8&#45;&gt;12 21 tr_y ≤ 9.0 mse = 1.0 samples = 14 value = 9.0 11&#45;&gt;21 22 tr_y ≤ 10.0 mse = 1.0 samples = 5 value = 10.0 11&#45;&gt;22 31 tr_y ≤ 8.0 mse = 1.0 samples = 8 value = 9.0 21&#45;&gt;31 32 tr_y ≤ 9.0 mse = 1.0 samples = 6 value = 8.0 21&#45;&gt;32 33 tr_y ≤ 8.0 mse = 0.0 samples = 6 value = 9.0 31&#45;&gt;33 34 tr_y ≤ 9.0 mse = 0.0 samples = 2 value = 10.0 31&#45;&gt;34 93 tr_y ≤ 8.0 mse = 0.0 samples = 3 value = 9.0 33&#45;&gt;93 94 tr_y ≤ 8.0 mse = 0.0 samples = 3 value = 9.0 33&#45;&gt;94 97 tr_y ≤ 8.0 mse = 0.0 samples = 2 value = 9.0 93&#45;&gt;97 98 mse = &#45;0.0 samples = 1 value = 10.0 93&#45;&gt;98 155 mse = 0.0 samples = 1 value = 9.0 97&#45;&gt;155 156 mse = 0.0 samples = 1 value = 9.0 97&#45;&gt;156 95 mse = 0.0 samples = 1 value = 8.0 94&#45;&gt;95 96 tr_y ≤ 8.0 mse = 0.0 samples = 2 value = 9.0 94&#45;&gt;96 145 mse = 0.0 samples = 1 value = 9.0 96&#45;&gt;145 146 mse = 0.0 samples = 1 value = 9.0 96&#45;&gt;146 53 mse = 0.0 samples = 1 value = 11.0 34&#45;&gt;53 54 mse = &#45;0.0 samples = 1 value = 9.0 34&#45;&gt;54 35 mse = 0.0 samples = 1 value = 7.0 32&#45;&gt;35 36 tr_y ≤ 9.0 mse = 1.0 samples = 5 value = 9.0 32&#45;&gt;36 37 tr_y ≤ 9.0 mse = 1.0 samples = 2 value = 9.0 36&#45;&gt;37 38 tr_y ≤ 9.0 mse = 0.0 samples = 3 value = 8.0 36&#45;&gt;38 39 mse = 0.0 samples = 1 value = 8.0 37&#45;&gt;39 40 mse = 0.0 samples = 1 value = 10.0 37&#45;&gt;40 85 tr_y ≤ 9.0 mse = 0.0 samples = 2 value = 9.0 38&#45;&gt;85 86 mse = 0.0 samples = 1 value = 8.0 38&#45;&gt;86 87 mse = 0.0 samples = 1 value = 8.0 85&#45;&gt;87 88 mse = 0.0 samples = 1 value = 9.0 85&#45;&gt;88 23 mse = 0.0 samples = 1 value = 11.0 22&#45;&gt;23 24 tr_y ≤ 10.0 mse = 0.0 samples = 4 value = 9.0 22&#45;&gt;24 73 mse = 0.0 samples = 1 value = 9.0 24&#45;&gt;73 74 tr_y ≤ 10.0 mse = 0.0 samples = 3 value = 9.0 24&#45;&gt;74 111 mse = 0.0 samples = 1 value = 10.0 74&#45;&gt;111 112 tr_y ≤ 10.0 mse = 0.0 samples = 2 value = 9.0 74&#45;&gt;112 113 mse = 0.0 samples = 1 value = 9.0 112&#45;&gt;113 114 mse = &#45;0.0 samples = 1 value = 9.0 112&#45;&gt;114 Balanced Case . In the balanced case, I have set max_leaf_nodes=10. This leads to a nice decision tree implementation with better generalization power than both above cases, which could be confirmed by seeing training and validation losses. . m = DecisionTreeRegressor(max_leaf_nodes=10) m.fit(tr_x, tr_y); fig, ax = plt.subplots(figsize=(16,8)) ax.scatter(x,y, marker=&#39;+&#39;, label=&#39;actual data&#39;) ax.scatter(tr_x, m.predict(tr_x), label=&#39;predicted data on training dataset&#39;) ax.scatter(val_x, m.predict(val_x), label=&#39;predicted data on validation dataset&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;y&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:15}) . &lt;matplotlib.legend.Legend at 0x7f9bb1e37860&gt; . It seems like a good fit because it&#39;s neither overfitting nor underfitting. . Below is the training and validation losses and complete decision tree as generated by the algorithm. . print(f&#39;Training rmse is {m_rmse(m, tr_x, tr_y)}, and validation rmse is {m_rmse(m, val_x, val_y)}&#39;) draw_tree(m, pd.DataFrame([tr_x,tr_y], columns=[&#39;tr_y&#39;])) . Training rmse is 0.7629283571148474, and validation rmse is 1.0146270166850742 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 tr_y ≤ 5.0 mse = 10.0 samples = 80 value = 5.0 1 tr_y ≤ 3.0 mse = 2.0 samples = 36 value = 3.0 0&#45;&gt;1 True 2 tr_y ≤ 7.0 mse = 3.0 samples = 44 value = 8.0 0&#45;&gt;2 False 5 tr_y ≤ 1.0 mse = 1.0 samples = 21 value = 1.0 1&#45;&gt;5 6 tr_y ≤ 4.0 mse = 1.0 samples = 15 value = 4.0 1&#45;&gt;6 9 tr_y ≤ 0.0 mse = 1.0 samples = 11 value = 1.0 5&#45;&gt;9 10 mse = 0.0 samples = 10 value = 2.0 5&#45;&gt;10 15 mse = 0.0 samples = 2 value = 2.0 9&#45;&gt;15 16 mse = 0.0 samples = 9 value = 1.0 9&#45;&gt;16 13 mse = 0.0 samples = 6 value = 3.0 6&#45;&gt;13 14 mse = 0.0 samples = 9 value = 5.0 6&#45;&gt;14 3 tr_y ≤ 6.0 mse = 0.0 samples = 13 value = 6.0 2&#45;&gt;3 4 tr_y ≤ 8.0 mse = 2.0 samples = 31 value = 9.0 2&#45;&gt;4 17 mse = 0.0 samples = 9 value = 5.0 3&#45;&gt;17 18 mse = 0.0 samples = 4 value = 6.0 3&#45;&gt;18 7 mse = 1.0 samples = 11 value = 8.0 4&#45;&gt;7 8 tr_y ≤ 10.0 mse = 1.0 samples = 20 value = 9.0 4&#45;&gt;8 11 mse = 1.0 samples = 19 value = 9.0 8&#45;&gt;11 12 mse = &#45;0.0 samples = 1 value = 11.0 8&#45;&gt;12 Extrapolation problem . The decision tree suffers from a serious drawback when trying to predict them on data outside the domain of the current dataset. Suppose we have split the dataset into training and validation such as included the first 80 datapoints in the training and the remaining 30 datapoints in the validation dataset, like: . tr_x, tr_y = x[:80], y[:80] val_x, val_y = x[80:], y[80:] . m = DecisionTreeRegressor(max_leaf_nodes=10) tr_x, tr_y = x[:80], y[:80] val_x, val_y = x[80:], y[80:] tr_x = tr_x.reshape(80,1) val_x = val_x.reshape(30,1) m.fit(tr_x, tr_y); fig, ax = plt.subplots(figsize=(16,8)) ax.scatter(x,y, marker=&#39;+&#39;, label=&#39;actual data&#39;) ax.scatter(tr_x, m.predict(tr_x), label=&#39;predicted data on training dataset&#39;) ax.scatter(val_x, m.predict(val_x), label=&#39;predicted data on validation dataset&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;y&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:15}) . &lt;matplotlib.legend.Legend at 0x7f9bc105fc50&gt; . In the above figure, because the validation data is in the range $x&gt;7.2$ something, and training data has only seen datapoints which are in the range $0 leq x leq7.2$, therefore validation data is out of the domain, and hence a poor extrapolation is done by decision trees. Because of this problem, and also high variance in predictions, a single decision tree is rarely used in practice. High Variance means high variance in the predictions, and it is because a little up and down in the training data could have changed the decision tree completely, and so the predictions will vary. A linear regression model has much less variance than a decision tree but bias is also higher than a decision tree. . Conclusion . We have covered the most basic ML method for tabular data modeling. In the next parts, I will cover Random Forests and some DL methods. Also, there is no point in training a decision tree model on the Kaggle dataset because it will give poor results as the data is complex, and it needs some more sophisticated algorithms. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/11/10/DLforCNN.html",
            "relUrl": "/2020/11/10/DLforCNN.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Optimization Algorithms Part II",
            "content": "Introduction . In the last post, we have discussed some widely used optimization algorithms for updating parameters in a Neural Network. All the covered algorithms need a good learning rate $ alpha$, otherwise, these algorithms won&#39;t work as desired, and finding the learning rate (one of the most important hyperparameter) is tricky and needs experimenting with different learning rates on the training dataset which takes a lot of computational power. . The other part of optimization algorithms comes under Adaptive Learning Rate Methods. The super benefit of these that they don&#39;t need fine-tuning of learning rates, it adapts learning rates to the parameters, performing updates by itself. . Adagrad . Adagrad algorithm does larger updates for infrequent parameters and smaller updates for frequent parameters. So it&#39;s beneficial for sparse data. Mathematically, . $x_{t+1} = x_t - frac{ alpha}{ sqrt{grad _squared + epsilon}}. frac{ partial{L}}{ partial{x_t}}$ . Here, $grad _squared$ is the square of the gradients for a parameter from the beginning, $ epsilon$ is usually of the order $1e-8$ to prevent division by zero error. Here $ alpha$ is the learning rate which is usually fixed at $0.01$. For frequent parameters, their gradients tend to grow bigger in squared sum than infrequent parameters. Therefore $ frac{1}{grad _squared}$ will be bigger for infrequent and smaller for frequent parameters, which will scale the learning rate accordingly. It will also solve the zigzag path problem as discussed in last post. . One issue with the Adagrad algorithm is that over time $ frac{1}{grad _squared}$ will be very small, and so the net learning rate will be very low which will make the learning of the model slow. . RMSProp . RMSProp is similar to Adagrad and tries to solve the problems encountered by Adagrad. Instead of maintaining the sum of the square of gradients of a parameter from the beginning, it does it for some window of past gradients. It maintains the window by the running mean of the square of gradients with an exponentially decaying rate. Mathematically, . $grad _squared = gamma * grad _squared + (1 - gamma)*grad _squared$ . $ gamma$ is usually kept at $0.9$, or $0.99$. . Adam . Adam stands for Adaptive Moment Estimation, and it&#39;s the most widely used Adaptive Learning Rate Method. Like Adagrad and RMSProp, it also maintains exponentially decaying mean of past gradients squares, but it also maintains exponentially decaying mean of past gradients. Mathematically, . $m_{t} = beta_1m_{t-1} + (1- beta_1)g_{t}$ . $v_{t} = beta_2v_{t-1} + (1- beta_2)g_{t}^2$ . where $g_t$ is the gradient computed at time $t$. $ beta_1$ and $ beta_2$ are generally kept close to 1 like 0.9 and 0.99 respectively. Like in other methods, $m_0$ and $v_0$ are initialized to 0. One issue because of that is the successive values of $m$ and $v$ became biased towards 0. The reason for this is because at $t=0$, $m_1$ is $0.1 * g_1$ and $0.9 * 0$, therefore the initial values of $m_t$ became much smaller and only after sometime it the bias is reduced. This is explained in the code below. The bias correction term for each of $m_{t}$ and $v_{t}$ is $ frac{1}{1 - beta_1^{t}}$ and $ frac{1}{1 - beta_2^{t}}$ respectively. Therefore, . $ hat{m}_{t} = frac{m_{t}}{1 - beta_1^{t}}$ . $ hat{v}_{t} = frac{v_{t}}{1 - beta_2^{t}}$ . And for each parameter $x$, it&#39;s updated as: . $x_{t+1} = x_t - frac{ alpha}{ sqrt{ hat{v}_t + epsilon}} hat{m}_t$ . For random integers in [1, 100], m contains moving average with exponentially decaying weights, m_bias_corrected also contains exponentially decaying mean but with bias correction (Pardon me if code is not as expressive and clear as it should be, you can simply skip and just understand the plot). . y = np.random.randint(1,100,30) beta1 = 0.9 m = [] m_bias_corrected = [] m.append(0) m_bias_corrected.append(0) for idx, g in enumerate(y): val = beta1*m[idx] + (1-beta1)*g m.append(val) m_bias_corrected.append(val/(1 - beta1**(idx + 1))) fig, ax = plt.subplots(figsize = (20, 12)) ax.plot(range(y.shape[0]), y, linestyle = &#39;:&#39;, linewidth =3, marker = &#39;.&#39;, label = &#39;data&#39;, markersize = 10) ax.plot(range(y.shape[0]), m[1:], linestyle = &#39;:&#39;, linewidth = 3, marker = &#39;v&#39;, markersize = 10, label=&#39;uncorrected_bias&#39;) ax.plot(range(y.shape[0]), m_bias_corrected[1:], linestyle = &#39;:&#39;, linewidth = 3, marker = &quot;o&quot;, markersize = 10, label=&#39;corrected_bias&#39;) ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(5)) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.set_xlabel(&quot;iterations&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;values&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.legend(prop={&quot;size&quot;:20}) . &lt;matplotlib.legend.Legend at 0x7f1659e4eb00&gt; . The orange line is uncorrected exponential moving average, and green is the corrected one. You can see for initial iterations the gap between them is quite big. This gap is reducing as we are moving to later iterations. The reason for these smaller values in the orange one is because 0.9 weight is given to 0 for $x_1$. This gap after 100s of iterations becomes negligible. . Conclusion . In the 2 part post, I have discussed the most commonly used non-adaptive and adaptive algorithms. There is a clear benefit of using adaptive ones because of no need to fine-tune the learning rate, also if the data is sparse then adaptive learning methods work better. RMSProp is an extension to Adagrad which deals with diminishing gradient values for the later iterations. Adam is a further improvement over RMSProp because of the bias correction, and therefore might be the best overall choice. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/11/03/DLforCNN.html",
            "relUrl": "/2020/11/03/DLforCNN.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Optimization Algorithms Part I",
            "content": "Introduction . There are millions of parameters in a typical deep neural network, and these parameters are initialized with random values. Now as the training proceeds, as mentioned in the previous post, we fed the training examples into the network and compute loss using a loss function, and take gradients of loss w.r.t. all the parameters so as to update them. Now with the updated network parameters we again fed the training data, and this process repeats time and again until the loss converges. The part where we update the parameters of the network is the focus of today&#39;s post. . To update a parameter means to change it&#39;s value so as to minimize the loss we incurred. It is an optimization problem where the objective function is the loss function $L(w)$, $w$ is the parameter we are updating, and ideally, we want to find the global minima of loss function $L$ for parameter w, if such global minima exist, otherwise find local minima. One such method to find a minima. as mentioned in the previous post, as Gradient Descent (GD) also known as Vanilla Gradient Descent, where we take a small step in the direction of the negative gradient ($- frac{ partial{L}}{ partial{w}}$). There are numerous issues with GD, like: . Too much memory is required to feed all the training examples at once to the model. | What if we reached a saddle point (a point which is not local extremum but having gradient 0, please refer wikipedia), then gradient will be zero there and no updation will happen. | Because of such issues, it&#39;s hardly used in practice. Other optimization algorithms have fundamental ideas the same as gradient descent but they encounter the issues faced by it. . Stochastic Gradient Descent (SGD) . It&#39;s a computational improvement over gradient descent as rather than feeding the whole training data it feeds data in batches. So if there are n training examples, then the batch size $b$ could be $1 leq b leq n$. Batch-size is chosen based on memory availability. Mathematically, . $x_{t+1} = x_t - alpha. frac{ partial{L}}{ partial{x_t}}$ . Some of the issues with SGD are: . Not whole data is fed at once, so gradients will be noisy. By noisy I mean that because each batch will have a different set of examples, so they try to change the parameters in different directions, therefore there won&#39;t be a smooth curve to the minima, as shown below. | . . What if the rate of change of loss is different in different directions. For example, as shown below, the contours of loss function form an ellipse, and function changes faster along the y-axis than the x-axis. Therefore the magnitude of the gradient along the y-axis will be more than the x-axis, and so the net direction of the gradient will align more with the y-axis. Therefore, the negative of the gradient will also align more with the y-axis, and hence the SGD will follow a zigzag path, which will take longer to converge. | . . Issues with saddle points still exists. | . SGD + Momentum . In SGD with Momentum, at any time t+1, the direction is not determined by $x_t$, but all the gradients from the beginning. Mathematically, if $v_0 = 0$ then, . $v_{t+1} = rho v_t + frac{ partial{L}}{ partial{x_t}}$ . $x_{t+1} = x_t - alpha.v_{t+1}$ . typically, $ rho$=0.9, or 0.99. $v_t$ is known as velocity which is actually the running mean of gradients. Velocity at a point has exponentially decaying weighted sum of previous gradients (older the gradient is less is its weight). Therefore it&#39;s less noisy than SGD because gradients of other batches are also added. . Also in case of different rates of change of loss along with different dimensions, velocity gives better direction, plus the dimension along which there is less rate of change of loss function will have better velocity than in a normal SGD. So convergence will be faster as the path will be less zigzag. The velocity increases for dimension whose gradients point in the same direction and reduces for dimensions whose updates are in different directions. Therefore there will be less oscillation while converging. . It won&#39;t get stuck in saddle point because of velocity. Also, it will overshoot local minima but will come back and approach the minima again. . Nesterov Momentum . One issue with SGD+Momentum is that the gradients could accumulate which leads to a high gradient in a particular direction, which is not good because it could surpass the local minima, also convergence will be slower as it has to move back and forth to reach a local minima. Imagine like a ball is running downhill, then SGD+Momentum could be thought of as ball accumulating very high velocity, so could surpass a local minima and will try to climb an uphill. Nesterov Momentum update tries to correct this &quot;blindly following the slope&quot; nature of SGD+Momentum by calculating the slope of the future point, and moving in the -ve of net gradient direction. Mathematically, . $v_{t+1} = rho v_t + alpha nabla_{x_t}L(x_t - rho v_t)$ . $x_{t+1} = x_t - v_{t+1}$ . So here, firstly gradient of loss function L w.r.t to $x_t$ is computed at the approximate future point $x_t - rho v_t$ (it&#39;s approximate because actual future point is $x_t - rho v_t -$ gradient_of_L_w.r.t_x_t), so we have gradient knowledge of the next point. Now we add accumulated velocities to it and make an update on x. . This anticipatory update prevents us from going too fast and results in better updates, which has significantly increased performance on DL models. Now, we are able to adapt our updates to the slope of our error function and speed up SGD in turn. . SGD+Momentum and Nesterov Momentum updates are shown below in the figure. SGD+M computes gradient first (small blue vector) and then takes a step in the net direction of -ve accumulated gradients (big blue vector). Whereas, Nesterov goes in the direction of previously accumulated gradients (big brown vector), then finds gradient at approximate next point, and takes a step in -ve gradient direction (small red vector), so that the resultant direction (big green vector) is different from SGD+M. . . Conclusion . We have covered Vanilla GD, SGD, SGD+Momentum, and Nesterov Momentum algorithms. All these optimization algorithms need a fine learning rate $ alpha$. If $ alpha$ too big then it will surpass the local minima or even diverge, if it&#39;s too small then convergence will be very slow. In Part II, we&#39;ll look at adaptive optimization algorithms, that will manage the learning rate for every parameter by itself, which is a very huge benefit over non-adaptive algorithms we have covered so far. .",
            "url": "https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html",
            "relUrl": "/2020/10/28/DLforCNN.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Activation Function",
            "content": "Introduction . We have seen in the last post that each neuron has weights, bias, and an activation function associated with it. It receives inputs $x_i$, each input $x_i$ is multiplied with weight $w_i$, and bias $b$ is added to the final product, therefore $sum = Sigma{(w_i.x_i)} + b$. Now activation function $ phi()$ is applied on the sum, so $y = phi( Sigma{(w_i.x_i)} + b )$. . If $w$ is a vector where $i^{th}$ value is $w_i$, and $x$ is vector where $i^{th}$ value is $x_i$, then $ Sigma{(w_i.x_i)} + b$ could be written as $w.x^T + b$. There output from a neuron is $ phi( w.x^T + b)$. . Need for the activation function . Suppose there are two neurons, one after another, and the output of the first is input to the second neuron, and there are no activation functions involved. Then, . $y_1 = w_1.x^T + b_1$ . $y_2 = w_2.{y_1}^T + b_2$ . If we substitute $y_1$ in second equation RHS then these both could be written together in a single equation as: $y_2 = w.x^T + b$, where $w = w_2.w_1$ and $b = w_2.b_1 + b_2$. . Therefore, we have replaced 2 linear layers with a single linear layer by changing the weights and bias as mentioned. This could be done even if we have 100s of linear layers stacked upon one another, thus, no matter how deep our network is we can re-write it as a single layer network. If we introduce a non-linear layer in between every 2 linear layers, then we can&#39;t replace them with a single neuron. Now each linear layer is actually somewhat decoupled from the other ones and can do its own useful work. . The addition of a non-linear layer or an activation function increases the power of a neural network. By power I mean our network could approximate any function (neural network could be seen as a function which maps an input $x$ to an output $y$), with accuracy as good as we want, all we have to do is to find the right set of weights and biases to do it. This fact is mathematically proven as a universal approximation theorem. . A single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices and few layers.$^{**}$ . Some commonly used activation functions . Sigmoid . A sigmoid function is defined as: . $sigmoid(x) = frac{1}{1+e^{-x}}$ . x = np.linspace(-7,7,100) y = 1/(1 + np.exp(-x)) fig,ax = plt.subplots(figsize = (5,5)) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;f(x)&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.plot(x, y, color=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5f5161fba8&gt;] . So, a sigmoid function squashes any real number in the range $(0,1)$, therefore, interpretable as probabilities. One of the big disadvantages of the sigmoid function is that its output is not zero-centered, which will be input to the next layer. . The problem if input data is not zero-centered data: . Mostly input data is +ve, so if $a = WX + b$, and $X$ is +ve, then the gradient of $W$ is +ve, or -ve, depending on the value $ frac{ partial{L}}{ partial{a}}$. So if optimal $W$ has both +ve and -ve values, but we end up searching for sub-optimal $W$. in case of 2-d, if $W_{opt}$ lies in $2^{nd}$ or $4^{th}$ quadrant, but we are only searching only in $1^{st}$ and $3^{rd}$ quadrant. . Another issue that saturated values kill the gradients during backpropagation, means that when input is a big positive or a negative number, then the slope of sigmoid in that region is very close to 0. So, in backpropagation, it gives a gradient close to 0 to its previous layers, and therefore very small or practically no weight-updation is performed on the network, thus network stops training. This is vanishing gradient problem. . Tanh . This is a hyperbolic tangent function with the range in (-1, 1), which makes it zero centered, a benefit over sigmoid. . y = np.tanh(x) fig,ax = plt.subplots(figsize = (5,5)) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;f(x)&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.plot(x, y, color=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5f5162e390&gt;] . In tanh also, saturated values kill the gradients during backpropagation, and therefore vanishing gradient problem still exists. . RELU . It is the most widely used activation function which is surprisingly mathematically simpler than sigmoid, or tanh, and mostly gives better results than them. It is defined as: . $RELU(x) = max(x,0)$ . y = np.maximum(x,0) fig,ax = plt.subplots(figsize = (5,5)) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;f(x)&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.plot(x, y, color=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5f5155bba8&gt;] . Some of the key points regarding RELU are: . Doesn’t saturate in the +ve region. | Computationally efficient to compute. | Convergence is faster. | Not zero centered outputs. | Gradient saturation in the negative half. | RELU has one major issue that it suffers from the problem of dying RELU, in which a RELU unit always outputs 0. As gradient is 0 when output is 0, therefore no weights updation happens, so the weights remain the same and RELU is very less likely to recover from this during the course of training. Dying RELU happens when there is a large grad flow and $W$ becomes big -ve numbers. . Leaky RELU . Tries to solve the dying RELU problem by having some slop for -ve input. It is defined as: . $RELU(x) = max(x, alpha*x)$, where $ alpha$ is generally a small +ve number in $(0,1)$. . y = np.maximum(x, 0.1*x) fig,ax = plt.subplots(figsize = (5,5)) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;f(x)&quot;, labelpad=5, fontsize=20, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) ax.plot(x, y, color=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f5f51603ac8&gt;] . All activation functions together . x = np.linspace(-5,5, 100) sig = 1/(1 + np.exp(-x)) tanh = np.tanh(x) x = np.linspace(-2,2, 100) relu = np.maximum(x, 0) lrelu = np.maximum(x, 0.1*x) fig,ax = plt.subplots(figsize = (10,7)) ax.set_xlabel(&quot;x&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.set_ylabel(&quot;f(x)&quot;, labelpad=5, fontsize=26, fontname=&#39;serif&#39;, color=&quot;blue&quot;) ax.plot(x, sig, marker = &#39;+&#39;, label = &#39;sigmoid&#39;) ax.plot(x, tanh, marker = &#39;.&#39;, label = &#39;tanh&#39;) ax.plot(x, relu, linestyle = &#39;--&#39;, label = &#39;RELU&#39;) ax.plot(x, lrelu, linestyle = &#39;:&#39;, label = &#39;Leaky RELU&#39;) ax.legend() ax.grid(which=&#39;major&#39;, axis=&#39;both&#39;, linestyle=&#39;:&#39;, linewidth = 1, color=&#39;b&#39;) . Conclusion . Activation Functions are the way to add non-linearity to our neural networks, and it is because of them that a fairly deep neural network is useful for complex functions representations, otherwise, it would be as good as a single layer neural network. I have included most of the commonly used activation functions, but there are many more. . $^{**}$credits: Fast.ai .",
            "url": "https://nitishsadire.github.io/DLblog/jupyter/2020/10/22/DLforCNN.html",
            "relUrl": "/jupyter/2020/10/22/DLforCNN.html",
            "date": " • Oct 22, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Deep Learning Introduction",
            "content": "Introduction . Deep Learning is a technique to extract and transform data from an input dataset, by using a deep network of neural network layers. By deep means that the number of layers is huge, could be as big as more than 100 layers. Layers in any deep neural network are in one of the following categories: . Input Layer: This is the layer where the input is applied to the network. | Hidden Layers: These are all the layers between the input layer and the output layer of a neural network. Each layer has multiple neurons (described in the next section). A neuron applies weights (linear function) to the input received and directs it through an activation function (non-linear function). Each hidden layer receives input as the output of the previous layer, applies transformations on the input, and gives output to the next layer. | Output Layer: This layer computes the output of the network in the format we want. E.g. In classification problem if there are C classes, then generally output layer gives a C length vector containing probabilities for each class, and we predict the class with the highest probability. | . The output of the neural network is compared against the true-output, and a loss-value is calculated using a loss function. . A loss function takes input as the network&#39;s output $( hat{y})$ and true-output$(y)$ and computes a scalar value which depicts our happiness or unhappiness with the result. E.g. If we have 5 classes, i.e. $C=5$, and we get $y=2$, but $ hat{y}=1$, it means that our network classifies input into class 2, but the ground truth showing input of class 1. To give feedback of our unhappiness to the network our loss value should be a high positive number. If $ hat{y} = y$ then our loss should be 0. High positive loss value means unhappiness and vice-versa because the network tries to minimize the loss value, as we will see in the next sections. . An image of a neural network, with 3 hidden layers (which is not so deep) is shown below. Here each node is a neuron and edges are weights. . . Neuron . A neuron is the fundamental block of a neural network. Each neuron has weights, bias, and an activation function associated with it, as shown in the figure below. It receives inputs $x_i$, each input $x_i$ is multiplied with weight $w_i$, and bias $b$ is added to the final product, therefore $sum = Sigma{(w_i.x_i)} + b$. Now activation function $ phi()$ is applied on the sum, so $y = phi( Sigma{(w_i.x_i)} + b )$. Some most commonly used activation functions are RELU, sigmoid, etc. . . Parameters Updation . When we feed an input into the neural network then it gives output $ hat{y}$. Let&#39;s have $y$ as ground truth label, and loss is $L = f(y, hat{y})$, where $f()$ is our loss function. We know that layer $L_i$ takes input as an output of layer $L_{i-1}$, which in turn takes input from the output of layer $L_{i-2}$, and so on. The point is that layer $L_i$ output depends on all the layers before it. Therefore the final neural network output $y$ could be thought of as a complex function taking all the network parameters (weights and bias of all neurons of all the layers) as input to that function. Mathematically, if $N()$ is a neural network function involving all parameters, and input is x, then loss $L = f(y, N(x))$ . Now we can compute derivatives of L w.r.t. to each parameter of neural network, $ frac{ partial{L}}{ partial(p)}$, for all parameters p of the network. $ frac{ partial{L}}{ partial(p)}$ gives the direction of the steepest ascent of the loss L w.r.t. to parameter p, which means the direction in which if we little bit change p then the value of L will increase the most. Therefore if we move p in the exact opposite direction then that will be the steepest descent direction, and so L will decrease the most. So, we can update parameter p as: $p = p - alpha frac{ partial{L}}{ partial(p)}$, where $ alpha$ is known as the learning rate, the length of the step we have taken in the steepest descent direction. . This is known as the classic Gradient Descent Algorithm for parameters updation. We can update all the parameters in a similar way, i.e. by computing gradient of loss L w.r.t. to a parameter, and then applying Gradient Descent Algorithm . Network Training . Suppose we have set of n training examples as $ {(x_1, y_1), (x_2, y_2), ... , (x_n, y_n) }$, where $x_i$ is the $i^{th}$ training example and $y_i$ is the true class label for $i^{th}$ training example. We can initialize all the network parameters with randomly small values, and update them after each iteration. The training steps could be defined as: . Computing neural network output on $x_1, x_2, ..., x_n$. | Computing loss as $f(y_1, hat{y_1}), f(y_2, hat{y_2}), ..., f(y_n, hat{y_n}) $, where $ hat{y_1}$ is class predicted by the neural network, and taking average value of loss | We compute gradients of average loss w.r.t. all the network parameters and update their values so as to minimize the loss. | . Keep on repeating the above steps until the loss converges. Going through all the training examples for once is known as 1 epoch. We can continue to train for multiple epochs until the loss converges. . Once training is done, we&#39;ll end up with such network weights which are far better than initial random weights in prediction, and we can use the same weights for inference on new unseen data. . Conclusion . I have covered a very basic understanding of deep learning models and terminology. In practice, the models which are deployed in production are very advanced and different, but the fundamental ideas remain the same. We will delve into a lot more deep learning topics in later posts. .",
            "url": "https://nitishsadire.github.io/DLblog/jupyter/2020/10/14/DLforCNN.html",
            "relUrl": "/jupyter/2020/10/14/DLforCNN.html",
            "date": " • Oct 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Nitish Sandhu, M.Tech from Computer Science and Automation Department, IISc (2018), B.Tech from Computer Science Engineering Department, NIT Kurukshetra (2015). .",
          "url": "https://nitishsadire.github.io/DLblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nitishsadire.github.io/DLblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}