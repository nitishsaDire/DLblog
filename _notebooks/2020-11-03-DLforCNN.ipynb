{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation Algorithms Part II\n",
    "> Some most commonly used optimisation algorithms for updating network parameters, and their advantages/disadvantages.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In the [last post](https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html), we have discussed some widely used optimization algorithms for updating parameters in a Neural Network. All the covered algorithms need good learning rate $\\alpha$, otherwise these algorithms won't work as desired, and finding learning rate (one of the most important hyperparameter) is tricky and needs experimenting different learning rates on the training dataset which takes lot of computational power.\n",
    "\n",
    "The other part of optimization algorithms come under Adaptive Learning Rate Methods. The super benefit of them that they don't need fine tuning of learning rates, it adapts learning rates to the parameters, performing updates by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Adagrad algorithm does larger updates for infrequent parameters and smaller updates for frequent parameters. So it's beneficial for sparse data. Mathematically,\n",
    "\n",
    "$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{grad\\_squared + \\epsilon}}.\\frac{\\partial{L}}{\\partial{x_t}}$\n",
    "\n",
    "Here, $grad\\_squared$ is the square of the gradients for a parameter from the beginning, $\\epsilon$ is usually of the order $1e-8$ to prevent division by zero error. Here $\\alpha$ is the learning rate which is usually fixed at $0.01$. For frequent parameters, their gradients tend to grow bigger in squared sum than infrequent parameters. Therefore $\\frac{1}{grad\\_squared}$ will be bigger for infrequent and smaller for frequent parameters, which will scale the learning rate accordingly. It will also solves the zigzag path problem as discussed in [last post](https://nitishsadire.github.io/DLblog/2020/10/28/DLforCNN.html).\n",
    "\n",
    "One issue with Adagrad algorithm is that over the time $\\frac{1}{grad\\_squared}$ will be very small, and so the net learning rate will be very small which will make learning of the model slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "RMSProp is similar to Adagrad, and tries to solve the problems encountered by Adagrad. Instead of maintaining sum of the square of gradients of a parameter from the beginning, it does it for some window of past gradients. It maintains the window by the running mean of the square of gradients with exponentially decaying rate. Mathematically,\n",
    "\n",
    "$grad\\_squared = \\gamma * grad\\_squared + (1 - \\gamma)*grad\\_squared$ \n",
    "\n",
    "$\\gamma$ is usually kept at $0.9$, or $0.99$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
