{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML/DL techniques for Tabular Modelling\n",
    "> Some most commonly used optimization algorithms for updating network parameters, and their advantages/disadvantages.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitish/miniconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# !pip install -Uqq fastbook\n",
    "\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *\n",
    "from kaggle import api\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from dtreeviz.trees import *\n",
    "from IPython.display import Image, display_svg, SVG\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "# api.competition_download_cli('bluebook-for-bulldozers')\n",
    "# file_extract('bluebook-for-bulldozers.zip')\n",
    "# df = pd.read_csv('/home/nitish/Downloads/bluebook-bulldozers/TrainAndValid.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Tabular Modelling takes data in the form of table, where generally we want to learn about a column's value from all the other columns' values. The column we want to learn is known as dependent variable and others are known as independent variabls. The learning could be both like a classification problem or regression problem. We will look into various machine learning models such as decision trees, random forests, etc, also we'll look for what deep learning has to offer in tabular modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "I will be using [Kaggle competition](https://www.kaggle.com/c/bluebook-for-bulldozers) dataset on all the models, so that it will be easier to udnerstand and compare different models. I have loaded it into a dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesID</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>MachineID</th>\n",
       "      <th>ModelID</th>\n",
       "      <th>...</th>\n",
       "      <th>Blade_Type</th>\n",
       "      <th>Travel_Controls</th>\n",
       "      <th>Differential_Type</th>\n",
       "      <th>Steering_Controls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139246</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>999089</td>\n",
       "      <td>3157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Conventional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139248</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>117657</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Conventional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139249</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>434808</td>\n",
       "      <td>7009</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1139251</td>\n",
       "      <td>38500.0</td>\n",
       "      <td>1026470</td>\n",
       "      <td>332</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1139253</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1057373</td>\n",
       "      <td>17311</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesID  SalePrice  MachineID  ModelID  ...  Blade_Type  Travel_Controls  \\\n",
       "0  1139246    66000.0     999089     3157  ...         NaN              NaN   \n",
       "1  1139248    57000.0     117657       77  ...         NaN              NaN   \n",
       "2  1139249    10000.0     434808     7009  ...         NaN              NaN   \n",
       "3  1139251    38500.0    1026470      332  ...         NaN              NaN   \n",
       "4  1139253    11000.0    1057373    17311  ...         NaN              NaN   \n",
       "\n",
       "   Differential_Type  Steering_Controls  \n",
       "0           Standard       Conventional  \n",
       "1           Standard       Conventional  \n",
       "2                NaN                NaN  \n",
       "3                NaN                NaN  \n",
       "4                NaN                NaN  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key fields are in train.csv are:\n",
    "\n",
    "- SalesID: the uniue identifier of the sale\n",
    "- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n",
    "- saleprice: what the machine sold for at auction (only provided in train.csv)\n",
    "- saledate: the date of the sale\n",
    "\n",
    "For this competition, we need to predict the log of the sale price of bulldozers sold at auctions. We will try to build different ML and DL models which will be predicting $log$(sale price)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "A decision tree makes a split in data based on the values of a columns. For example, suppose we have data for different persons for their age, whether they eat healthy, whether they exercise, etc, and want to predict whether they are fit or unfit based on the data then we can use the following decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/blog5_1.png \"Credit:fast.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each level, the data at that level is divided into 2 groups for the next level, e.g. at first level, whether age<30 or not divides the whole dataset into 2 smaller datasets, and similarly data is splitted again until we reach leaf node of 2 classes: FIT or UNFIT. \n",
    "\n",
    "In the real world, data is way more complex containing lot of columns. E.g. in our dataframe df, there are 53 columns. So the question arises which column to chose for each split and what should be the value at which it is splitted. The answer is try for every column and each value present in a column for split. So if there are n columns and each column has x different values then we need to try n\\*x splits and chose the best one on some criteria. When trying a split, then whole data will be divided into 2 groups for that level, so we can take the average of sale price of a group as predicted sale price for all the rows in that group, and can calculate rmse distance between predictions and actual sale price. This will give us a number which if bigger tells our predictions are far from actual sale price and vice-versa. So the algorithm for building a decision tree could be written as:\n",
    "1. Loop through all the columns in the training dataset.\n",
    "1. Loop through all the possible values for a column. If the column contains categorical data then chose condition as \"equal to\" a category and \"not equal to\" a category. If the column contains continuos data then for all the distinct values split on \"less than equal to\" and \"greater than\" the value.\n",
    "1. Find average sale price for each of the group, this is our prediction. Calculate rmse from the actual values of sale price.\n",
    "1. The rmse of a split could be set as sum of rmse for all groups after split.\n",
    "1. After looping though all the columns and all possible splits for each column chose the split with least rmse.\n",
    "1. Continue the same process recursively on the child groups until some stopping criteria is reached like maximum number of leaf node, minimum number of data items per group, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is given an example of decision tree. In the root node, value is simply average of all the training dataset which would be the most simple prediction we could calculate for a new datapoint is to simply give prediction of 10.1 everytime. Mean Square Error (mse) is 0.48, and there are total 404710 samples, which is actually the total number of samples in training dataset.\n",
    "\n",
    "Now for the split, it would have tried many all the columns at all the possible values, and it came with $Coupler\\_System \\leq 0.5$ split. This would split the whole dataset into two smaller datasets. When condition is false it resulted in 360847 samples, with mse of 0.42 and average value of 10.21. When condition is true it resulted in 43863 samples, with mse of 0.12 and average value of 9.21. It could be seen that this split has improved our prediction because now average mse is (0.42 + 0.12)/2 < 0.48.\n",
    "\n",
    "Similarly, splitting the \"True condition child\" on $YearMade \\leq 0.42$ further deacreases the mse, which means our predictions are further closer to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/blog5_2.png \"Credit:fast.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting in decision trees\n",
    "Underfitting in the decision trees will be when we make very few splits or no splits at all, e.g. in the root node the average value is 10.1 and if we use this value as prediction then it's clearly a very simple solution to an complex problem, which is an underfitting.\n",
    "\n",
    "Overfitting will be when there are way too many splits such that in extreme case there are one sample per leaf node, which is actually the model has memorize the training dataset. It is overfitting because although the mse will be 0 for the training dataset, it will be very high for validation dataset, as the model will fail to generalize on unseen datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
