{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimisation PART I\n",
    "> Explanation of mathematical concepts behind optmisation in ML/DL.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: ../images/neuron.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In a deep learning software, there is high amount of numerical computation involved. The deeper a network gets more  intesive the amount of numerical computation becomes. The rounding error at a one deeper layer could get amplified for the later layers, making the network learning wrong paramters. Because of the real numbers, underflow and overflow are two common issues occuring, for example in a softmax computation if value of $x_i$ is large positive number then $e^{x_i}$ could overflow, similarly if it's large negative number then $e^{x_i}$ could underflow. To overcome this, we subtract the max of $x$ from all $x_i$ and then only take softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Based Optimisation\n",
    "If there is a function $f(x)$, and we want to find value of $x$ where $f(x)$ is minimum, then calculus tells us that it happens where derivative or gradient of $f(x)$ w.r.t. $x$ is zero as shown in fig below. Gradient could be thought as linearly approximating the function $f(x)$ as $x$ changes to $x+\\epsilon$ (bad approximation for large $\\epsilon$):\n",
    "\n",
    "$f(x+\\epsilon) \\approx f(x) + \\epsilon .f'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/blog12_1.png \"Credit: Deep Learning book by Ian Goodfellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning involves optimisation, but the function is very complex such that it is not possible to get minima in a single step, so rather an iterative approach is followed, as we have discussed in Gradient Descent algorithm.\n",
    "Here, when I said \"minima\" I meant global minima, but there is also local minima, point where value of function is less than the value at neighbouring points. In general, the points where grad is zero are called as critical points, it could be any of the local/global minima maxima, saddle point (local minima along one direction and local maxima along other direction). In high dimensional functions (which are common in DL) there are lot of saddle points, local minima, and that what makes optimisation very difficult. \n",
    "\n",
    "### DIrection Derivative\n",
    "Directional derivative is the measure of gradient along a certain direction. E.g. if $f(x)$ is the function involved, then directional derivative along the direction $u$ is derivative of $f(x+\\alpha u)$ w.r.t $\\alpha$ measured at $\\alpha$=0. We know $\\frac{\\partial f(x+\\alpha u)}{\\partial \\alpha}$ is $u^T\\nabla f(x)$ at $\\alpha=0$. Therefore, the direction of steepest descent (direction along which grad is decreasing fastest) will be when $u$ points in the opposite direction of $\\nabla f(x)$, and this is the fundamental idea behind gradient descent.\n",
    "\n",
    "### Hessian\n",
    "Hessian is the derivative of gradient, so if gradient is d-dim vector, measuring change of function along each of the d directions, then hessian is the dxd matrix, where ij entry measuring change of gradient along direction i with little change along direction j. Hessian measures curvature of the function, and there could be positive (upward), negative (downward) or no curvature (straigh line). Therefore the value of $f(x)$ decreases more than expected, decreases as expected and increases for the curvature negative, zero, positive respectively, when we make step of $\\epsilon$ along a negative gradient direction, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/blog12_2.png \"Credit: Deep Learning book by Ian Goodfellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian matrix is symmetric, means there exists orthogonal eigen-vectors and real eigen-values. The second directional derivative along direction d is $d^THd$. This is very interesting result because if d is choosen to be a unit vector then the value of $d^THd$ will be maximum/minimum when d is eigenvector corresponding to maximum/minimum eigenvalue, and for all the directions other than those in eigenvectors its value could be writen as some combination of value for eigenvectors (like a vector is respresented as linear combination of the basis).\n",
    "\n",
    "Using taylor series quadratic approximation:\n",
    "\n",
    "$f(x - \\epsilon g) \\approx f(x) - \\epsilon g^Tg + \\frac{1}{2}\\epsilon^2g^THg$\n",
    "\n",
    "When $g^THg$ is zero or negative then higher is the step size $\\epsilon$ lesser will the $f(x)$. When $g^THg$ is positive then optimal step size could be derived.\n",
    "\n",
    "The value of grad along with value of hessian is used to confirm whether the critical point is local minima/maxima, or a saddle point. \n",
    "\n",
    "At a critical point one of the following things could happen:\n",
    "1. If all the eigenvalues of hessian matrix are greater than zero, means the value of $d^THd$ is positive along all the directions, therefore the point is local minima because it has positive curvature (upwards) along all the directions.\n",
    "\n",
    "1. If all the eigenvalues of hessian matrix are less than zero, means the value of $d^THd$ is negative along all the directions, therefore the point is local maxima because it has negative curvature (downwards) along all the directions.\n",
    "\n",
    "1. If some of the eigenvalues are positive and some are negative then it means along one direction the curvature is upwards, and along other direction curvature is downwards, therefore the point is a saddle point.\n",
    "\n",
    "\n",
    "The condition number of the matrix is defined as the ratio of largest absolute eigenvalue to the smallest absolute eigenvalue. For an hessian it tells how much the second directional derivatives differ along different directions. When condition number is high then gradient descent performs poorly because the gradient along one direction (aligning with maximum eigenvalue's eigenvector) changes rapidly and along other direction (aligning with miniimum eigenvalue's eigenvector) changes slowly. This makes difficult to choose stepsize, and therefore gradient descent converges in a zig-zag manner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
