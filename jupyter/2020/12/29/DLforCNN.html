<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Numerical Optimisation PART I | DLBlog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Numerical Optimisation PART I" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Explanation of mathematical concepts behind optmisation in ML/DL." />
<meta property="og:description" content="Explanation of mathematical concepts behind optmisation in ML/DL." />
<link rel="canonical" href="https://nitishsadire.github.io/DLblog/jupyter/2020/12/29/DLforCNN.html" />
<meta property="og:url" content="https://nitishsadire.github.io/DLblog/jupyter/2020/12/29/DLforCNN.html" />
<meta property="og:site_name" content="DLBlog" />
<meta property="og:image" content="https://nitishsadire.github.io/images/neuron.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-29T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://nitishsadire.github.io/DLblog/jupyter/2020/12/29/DLforCNN.html","@type":"BlogPosting","headline":"Numerical Optimisation PART I","dateModified":"2020-12-29T00:00:00-06:00","datePublished":"2020-12-29T00:00:00-06:00","image":"https://nitishsadire.github.io/images/neuron.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://nitishsadire.github.io/DLblog/jupyter/2020/12/29/DLforCNN.html"},"description":"Explanation of mathematical concepts behind optmisation in ML/DL.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DLblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nitishsadire.github.io/DLblog/feed.xml" title="DLBlog" /><link rel="shortcut icon" type="image/x-icon" href="/DLblog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DLblog/">DLBlog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DLblog/about/">About Me</a><a class="page-link" href="/DLblog/search/">Search</a><a class="page-link" href="/DLblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Numerical Optimisation PART I</h1><p class="page-description">Explanation of mathematical concepts behind optmisation in ML/DL.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-29T00:00:00-06:00" itemprop="datePublished">
        Dec 29, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/DLblog/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/nitishsaDire/DLblog/tree/master/_notebooks/2020-12-29-DLforCNN.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/DLblog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/nitishsaDire/DLblog/master?filepath=_notebooks%2F2020-12-29-DLforCNN.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/DLblog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/nitishsaDire/DLblog/blob/master/_notebooks/2020-12-29-DLforCNN.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/DLblog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Gradient-Based-Optimisation">Gradient Based Optimisation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#DIrection-Derivative">DIrection Derivative </a></li>
<li class="toc-entry toc-h3"><a href="#Hessian">Hessian </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-29-DLforCNN.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p>In a deep learning software, there is high amount of numerical computation involved. The deeper a network gets more  intesive the amount of numerical computation becomes. The rounding error at a one deeper layer could get amplified for the later layers, making the network learning wrong paramters. Because of the real numbers, underflow and overflow are two common issues occuring, for example in a softmax computation if value of $x_i$ is large positive number then $e^{x_i}$ could overflow, similarly if it's large negative number then $e^{x_i}$ could underflow. To overcome this, we subtract the max of $x$ from all $x_i$ and then only take softmax.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Based-Optimisation">
<a class="anchor" href="#Gradient-Based-Optimisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Based Optimisation<a class="anchor-link" href="#Gradient-Based-Optimisation"> </a>
</h2>
<p>If there is a function $f(x)$, and we want to find value of $x$ where $f(x)$ is minimum, then calculus tells us that it happens where derivative or gradient of $f(x)$ w.r.t. $x$ is zero as shown in fig below. Gradient could be thought as linearly approximating the function $f(x)$ as $x$ changes to $x+\epsilon$ (bad approximation for large $\epsilon$):</p>
<p>$f(x+\epsilon) \approx f(x) + \epsilon .f'(x)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/DLblog/images/copied_from_nb/images/blog12_1.png" alt="" title="Credit: Deep Learning book by Ian Goodfellow"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Deep learning involves optimisation, but the function is very complex such that it is not possible to get minima in a single step, so rather an iterative approach is followed, as we have discussed in Gradient Descent algorithm.
Here, when I said "minima" I meant global minima, but there is also local minima, point where value of function is less than the value at neighbouring points. In general, the points where grad is zero are called as critical points, it could be any of the local/global minima maxima, saddle point (local minima along one direction and local maxima along other direction). In high dimensional functions (which are common in DL) there are lot of saddle points, local minima, and that what makes optimisation very difficult.</p>
<h3 id="DIrection-Derivative">
<a class="anchor" href="#DIrection-Derivative" aria-hidden="true"><span class="octicon octicon-link"></span></a>DIrection Derivative<a class="anchor-link" href="#DIrection-Derivative"> </a>
</h3>
<p>Directional derivative is the measure of gradient along a certain direction. E.g. if $f(x)$ is the function involved, then directional derivative along the direction $u$ is derivative of $f(x+\alpha u)$ w.r.t $\alpha$ measured at $\alpha$=0. We know $\frac{\partial f(x+\alpha u)}{\partial \alpha}$ is $u^T\nabla f(x)$ at $\alpha=0$. Therefore, the direction of steepest descent (direction along which grad is decreasing fastest) will be when $u$ points in the opposite direction of $\nabla f(x)$, and this is the fundamental idea behind gradient descent.</p>
<h3 id="Hessian">
<a class="anchor" href="#Hessian" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hessian<a class="anchor-link" href="#Hessian"> </a>
</h3>
<p>Hessian is the derivative of gradient, so if gradient is d-dim vector, measuring change of function along each of the d directions, then hessian is the dxd matrix, where ij entry measuring change of gradient along direction i with little change along direction j. Hessian measures curvature of the function, and there could be positive (upward), negative (downward) or no curvature (straigh line). Therefore the value of $f(x)$ decreases more than expected, decreases as expected and increases for the curvature negative, zero, positive respectively, when we make step of $\epsilon$ along a negative gradient direction, as shown below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/DLblog/images/copied_from_nb/images/blog12_2.png" alt="" title="Credit: Deep Learning book by Ian Goodfellow"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Hessian matrix is symmetric, means there exists orthogonal eigen-vectors and real eigen-values. The second directional derivative along direction d is $d^THd$. This is very interesting result because if d is choosen to be a unit vector then the value of $d^THd$ will be maximum/minimum when d is eigenvector corresponding to maximum/minimum eigenvalue, and for all the directions other than those in eigenvectors its value could be writen as some combination of value for eigenvectors (like a vector is respresented as linear combination of the basis).</p>
<p>Using taylor series quadratic approximation:</p>
<p>$f(x - \epsilon g) \approx f(x) - \epsilon g^Tg + \frac{1}{2}\epsilon^2g^THg$</p>
<p>When $g^THg$ is zero or negative then higher is the step size $\epsilon$ lesser will the $f(x)$. When $g^THg$ is positive then optimal step size could be derived.</p>
<p>The value of grad along with value of hessian is used to confirm whether the critical point is local minima/maxima, or a saddle point.</p>
<p>At a critical point one of the following things could happen:</p>
<ol>
<li>
<p>If all the eigenvalues of hessian matrix are greater than zero, means the value of $d^THd$ is positive along all the directions, therefore the point is local minima because it has positive curvature (upwards) along all the directions.</p>
</li>
<li>
<p>If all the eigenvalues of hessian matrix are less than zero, means the value of $d^THd$ is negative along all the directions, therefore the point is local maxima because it has negative curvature (downwards) along all the directions.</p>
</li>
<li>
<p>If some of the eigenvalues are positive and some are negative then it means along one direction the curvature is upwards, and along other direction curvature is downwards, therefore the point is a saddle point.</p>
</li>
</ol>
<p>The condition number of the matrix is defined as the ratio of largest absolute eigenvalue to the smallest absolute eigenvalue. For an hessian it tells how much the second directional derivatives differ along different directions. When condition number is high then gradient descent performs poorly because the gradient along one direction (aligning with maximum eigenvalue's eigenvector) changes rapidly and along other direction (aligning with miniimum eigenvalue's eigenvector) changes slowly. This makes difficult to choose stepsize, and therefore gradient descent converges in a zig-zag manner.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="nitishsaDire/DLblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/DLblog/jupyter/2020/12/29/DLforCNN.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DLblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DLblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DLblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
